%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% datoteka diploma-FRI-vzorec.tex
%
%POZOR: ta verzija ne producira pdf datoteke v pdf/A formatu!!!
%namenjena je le za nalogo pri Diplomskem seminarju!
%
% vzorčna datoteka za pisanje diplomskega dela v formatu LaTeX
% na UL Fakulteti za računalništvo in informatiko
%
% na osnovi starejših verzij vkup spravil Franc Solina, maj 2021
% prvo verzijo je leta 2010 pripravil Gašper Fijavž
%
% za upravljanje z literaturo ta vezija uporablja BibLaTeX
%
% svetujemo uporabo Overleaf.com - na tej spletni implementaciji LaTeXa ta vzorec zagotovo pravilno deluje
%

\documentclass[a4paper,12pt,openright]{book}
%\documentclass[a4paper, 12pt, openright, draft]{book}  Nalogo preverite tudi z opcijo draft, ki pokaže, katere vrstice so predolge! Pozor, v draft opciji, se slike ne pokažejo!
 
\usepackage[utf8]{inputenc}   % omogoča uporabo slovenskih črk kodiranih v formatu UTF-8
\usepackage[slovene,english]{babel}    % naloži, med drugim, slovenske delilne vzorce
\usepackage[pdftex]{graphicx}  % omogoča vlaganje slik različnih formatov
\usepackage{fancyhdr}          % poskrbi, na primer, za glave strani
\usepackage{amssymb}           % dodatni matematični simboli
\usepackage{amsmath}           % eqref, npr.
\usepackage[pdftex, colorlinks=true,
						citecolor=black, filecolor=black, 
						linkcolor=black, urlcolor=black,
						pdfproducer={LaTeX}, pdfcreator={LaTeX}]{hyperref}
\usepackage{hyperxmp}
\usepackage[hyphens]{url}
\usepackage{csquotes}


\usepackage{color}
\usepackage{soul}
\usepackage{caption}
\captionsetup{font=small}
\let\oldtable\table
\let\endoldtable\endtable
\renewenvironment{table}[1][htbp] % redefine table environment
{
    \oldtable[#1]
    \footnotesize                     % sets all content in table smaller
}
{
    \endoldtable
}

\graphicspath{{../results/}}

\usepackage{float}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{makecell}
\definecolor{Apricot}{RGB}{251, 206, 177}

\usepackage[
backend=biber,
style=authoryear,
sorting=nty,
]{biblatex}


\addbibresource{literatura.bib} %Imports bibliography file


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	DIPLOMA INFO
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\ttitle}{Spodbujevano učenje na impulznih nevronskih mrežah}
\newcommand{\ttitleEn}{Reinforcement learning on spiking neural networks}
\newcommand{\tsubject}{\ttitle}
\newcommand{\tsubjectEn}{\ttitleEn}
\newcommand{\tauthor}{Matjaž Pogačnik}
\newcommand{\tkeywords}{impulzne nevronske mreže, spodbujevano učenje, R-STDP učenje, TD učenje}
\newcommand{\tkeywordsEn}{spiking neural networks, reinforcement learning, R-STDP learning, TD learning}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	HYPERREF SETUP
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\hypersetup{pdftitle={\ttitle}}
\hypersetup{pdfsubject=\ttitleEn}
\hypersetup{pdfauthor={\tauthor}}
\hypersetup{pdfkeywords=\tkeywordsEn}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% postavitev strani
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  

\addtolength{\marginparwidth}{-20pt} % robovi za tisk
\addtolength{\oddsidemargin}{40pt}
\addtolength{\evensidemargin}{-40pt}

\renewcommand{\baselinestretch}{1.3} % ustrezen razmik med vrsticami
\setlength{\headheight}{15pt}        % potreben prostor na vrhu
\renewcommand{\chaptermark}[1]%
{\markboth{\MakeUppercase{\thechapter.\ #1}}{}} \renewcommand{\sectionmark}[1]%
{\markright{\MakeUppercase{\thesection.\ #1}}} \renewcommand{\headrulewidth}{0.5pt} \renewcommand{\footrulewidth}{0pt}
\fancyhf{}
\fancyhead[LE,RO]{\sl \thepage} 
%\fancyhead[LO]{\sl \rightmark} \fancyhead[RE]{\sl \leftmark}
\fancyhead[RE]{\sc \tauthor}              % dodal Solina
\fancyhead[LO]{\sc Diplomska naloga}     % dodal Solina


\newcommand{\BibLaTeX}{{\sc Bib}\LaTeX}
\newcommand{\BibTeX}{{\sc Bib}\TeX}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% naslovi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  

\newcommand{\autfont}{\Large}
\newcommand{\titfont}{\LARGE\bf}
\newcommand{\clearemptydoublepage}{\newpage{\pagestyle{empty}\cleardoublepage}}
\setcounter{tocdepth}{1}	      % globina kazala

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% konstrukti
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
\newtheorem{izrek}{Izrek}[chapter]
\newtheorem{trditev}{Trditev}[izrek]
\newenvironment{dokaz}{\emph{Dokaz.}\ }{\hspace{\fill}{$\Box$}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% PDF-A
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% define medatata
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\def\Title{\ttitle}
\def\Author{\tauthor, matjaz.kralj@fri.uni-lj.si}
\def\Subject{\ttitleEn}
\def\Keywords{\tkeywordsEn}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% \convertDate converts D:20080419103507+02'00' to 2008-04-19T10:35:07+02:00
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\def\convertDate{%
    \getYear
}

{\catcode`\D=12
 \gdef\getYear D:#1#2#3#4{\edef\xYear{#1#2#3#4}\getMonth}
}
\def\getMonth#1#2{\edef\xMonth{#1#2}\getDay}
\def\getDay#1#2{\edef\xDay{#1#2}\getHour}
\def\getHour#1#2{\edef\xHour{#1#2}\getMin}
\def\getMin#1#2{\edef\xMin{#1#2}\getSec}
\def\getSec#1#2{\edef\xSec{#1#2}\getTZh}
\def\getTZh +#1#2{\edef\xTZh{#1#2}\getTZm}
\def\getTZm '#1#2'{%
    \edef\xTZm{#1#2}%
    \edef\convDate{\xYear-\xMonth-\xDay T\xHour:\xMin:\xSec+\xTZh:\xTZm}%
}

%\expandafter\convertDate\pdfcreationdate 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% get pdftex version string
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\newcount\countA
\countA=\pdftexversion
\advance \countA by -100
\def\pdftexVersionStr{pdfTeX-1.\the\countA.\pdftexrevision}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% XMP data
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
\usepackage{xmpincl}
%\includexmp{pdfa-1b}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% pdfInfo
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
\pdfinfo{%
    /Title    (\ttitle)
    /Author   (\tauthor, damjan@cvetan.si)
    /Subject  (\ttitleEn)
    /Keywords (\tkeywordsEn)
    /ModDate  (\pdfcreationdate)
    /Trapped  /False
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% znaki za copyright stran
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  

\newcommand{\CcImageCc}[1]{%
	\includegraphics[scale=#1]{cc_cc_30.pdf}%
}
\newcommand{\CcImageBy}[1]{%
	\includegraphics[scale=#1]{cc_by_30.pdf}%
}
\newcommand{\CcImageSa}[1]{%
	\includegraphics[scale=#1]{cc_sa_30.pdf}%
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\selectlanguage{slovene}
\frontmatter
\setcounter{page}{1} %
\renewcommand{\thepage}{}       % preprečimo težave s številkami strani v kazalu

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%naslovnica
 \thispagestyle{empty}%
   \begin{center}
    {\large\sc Univerza v Ljubljani\\%
%      Fakulteta za elektrotehniko\\% za študijski program Multimedija
%      Fakulteta za upravo\\% za študijski program Upravna informatika
      Fakulteta za računalništvo in informatiko\\%
%      Fakulteta za matematiko in fiziko\\% za študijski program Računalništvo in matematika
     }
    \vskip 10em%
    {\autfont \tauthor\par}%
    {\titfont \ttitle \par}%
    {\vskip 3em \textsc{DIPLOMSKO DELO\\[5mm]         % dodal Solina za ostale študijske programe
%    VISOKOŠOLSKI STROKOVNI ŠTUDIJSKI PROGRAM\\ PRVE STOPNJE\\ RAČUNALNIŠTVO IN INFORMATIKA}\par}%
     UNIVERZITETNI  ŠTUDIJSKI PROGRAM\\ PRVE STOPNJE\\ RAČUNALNIŠTVO IN INFORMATIKA}\par}%
%    INTERDISCIPLINARNI UNIVERZITETNI\\ ŠTUDIJSKI PROGRAM PRVE STOPNJE\\ MULTIMEDIJA}\par}%
%    INTERDISCIPLINARNI UNIVERZITETNI\\ ŠTUDIJSKI PROGRAM PRVE STOPNJE\\ UPRAVNA INFORMATIKA}\par}%
%    INTERDISCIPLINARNI UNIVERZITETNI\\ ŠTUDIJSKI PROGRAM PRVE STOPNJE\\ RAČUNALNIŠTVO IN MATEMATIKA}\par}%
    \vfill\null%
% izberite pravi habilitacijski naziv mentorja!
    {\large \textsc{Mentor}: prof. dr. Zoran Bosnić\par}%
%   {\large \textsc{Somentor}:  viš. pred./doc./izr. prof./prof. dr.  Martin Krpan \par}%
    {\vskip 2em \large Ljubljana, \the\year \par}%
\end{center}
% prazna stran
%\clearemptydoublepage      
% izjava o licencah itd. se izpiše na hrbtni strani naslovnice

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%copyright stran
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\thispagestyle{empty}

\vspace*{5cm}
{\small \noindent
To delo je ponujeno pod licenco \textit{Creative Commons Priznanje avtorstva-Deljenje pod enakimi pogoji 2.5 Slovenija} (ali novej\v so razli\v cico).
To pomeni, da se tako besedilo, slike, grafi in druge sestavine dela kot tudi rezultati diplomskega dela lahko prosto distribuirajo,
reproducirajo, uporabljajo, priobčujejo javnosti in predelujejo, pod pogojem, da se jasno in vidno navede avtorja in naslov tega
dela in da se v primeru spremembe, preoblikovanja ali uporabe tega dela v svojem delu, lahko distribuira predelava le pod
licenco, ki je enaka tej.
Podrobnosti licence so dostopne na spletni strani \href{http://creativecommons.si}{creativecommons.si} ali na Inštitutu za
intelektualno lastnino, Streliška 1, 1000 Ljubljana.

\vspace*{1cm}
\begin{center}% 0.66 / 0.89 = 0.741573033707865
\CcImageCc{0.741573033707865}\hspace*{1ex}\CcImageBy{1}\hspace*{1ex}\CcImageSa{1}%
\end{center}
}

\vspace*{1cm}
{\small \noindent
Izvorna koda diplomskega dela, njeni rezultati in v ta namen razvita programska oprema je ponujena pod licenco GNU General Public License,
različica 3 (ali novejša). To pomeni, da se lahko prosto distribuira in/ali predeluje pod njenimi pogoji.
Podrobnosti licence so dostopne na spletni strani \url{http://www.gnu.org/licenses/}.
}

\vfill
\begin{center} 
\ \\ \vfill
{\em
Besedilo je oblikovano z urejevalnikom besedil \LaTeX.}
\end{center}

% prazna stran
\clearemptydoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% stran 3 med uvodnimi listi
\thispagestyle{empty}
\
\vfill

\bigskip
\noindent\textbf{Kandidat:} Matjaž Pogačnik\\
\noindent\textbf{Naslov:} Spodbujevano učenje na impulznih nevronskih mrežah\\
% vstavite ustrezen naziv študijskega programa!
\noindent\textbf{Vrsta naloge:} Diplomska naloga na univerzitetnem programu prve stopnje Računalništvo in informatika \\
% izberite pravi habilitacijski naziv mentorja!
\noindent\textbf{Mentor:} prof. dr. Zoran Bosnić\\
% \noindent\textbf{Somentor:} isto kot za mentorja

\bigskip
\noindent\textbf{Opis:}\\
Kandidat naj preuči principe impulznih nevronskih mrež in pristope spodbujevanega učenja, primernih za takšne modele. Razvije in implementira izbran algoritem učenja ter ga eksperimentalno ovrednoti na izbranem problemskem okolju. Rezultate naj analizira in primerja z obstoječimi pristopi.

\vfill



\vspace{2cm}

% prazna stran
\clearemptydoublepage

% TODO: zahvala
% zahvala
\iffalse
\thispagestyle{empty}\mbox{}\vfill\null\it%
\noindent
Na tem mestu zapišite, komu se zahvaljujete za pomoč pri izdelavi diplomske naloge oziroma pri vašem študiju nasploh. Pazite, da ne boste koga pozabili. Utegnil vam bo zameriti. Temu se da izogniti tako, da celotno zahvalo izpustite.
\rm\normalfont

% prazna stran
\clearemptydoublepage
\fi

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% posvetilo, če sama zahvala ne zadošča :-)
%\thispagestyle{empty}\mbox{}{\vskip0.20\textheight}\mbox{}\hfill\begin{minipage}{0.55\textwidth}%
%Svoji dragi Alenčici.
%\normalfont\end{minipage}

% prazna stran
%\clearemptydoublepage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% kazalo
\pagestyle{empty}
\def\thepage{}% preprečimo težave s številkami strani v kazalu
\tableofcontents{}


% prazna stran
\clearemptydoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% seznam kratic

\chapter*{Seznam uporabljenih kratic}

\noindent\begin{tabular}{p{0.15\textwidth}|p{.36\textwidth}|p{.39\textwidth}}    % po potrebi razširi prvo kolono tabele na račun drugih dveh!
  {\bf kratica} & {\bf angleško}                              & {\bf slovensko} \\ \hline
  {\bf STDP} & Spike timing dependent plasticity & Sinaptična plastičnost odvisna od časovne razporeditve impulzov \\
  {\bf R-STDP} & Reward modulated spike timing dependent plasticity & Sinaptična plastičnost odvisna od nagrajevanja in časovne razporeditve impulzov \\
  {\bf TD}   & Temporal difference              & Časovna razlika \\
%  \dots & \dots & \dots \\
\end{tabular}


% prazna stran
\clearemptydoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% povzetek
\addcontentsline{toc}{chapter}{Povzetek}
\chapter*{Povzetek}

\noindent\textbf{Naslov:} \ttitle
\bigskip

\noindent\textbf{Avtor:} \tauthor
\bigskip

%\noindent\textbf{Povzetek:} 
\noindent 
%V vzorcu je predstavljen postopek priprave diplomskega dela z uporabo okolja \LaTeX. Vaš povzetek mora sicer vsebovati približno 100 besed, ta tukaj je odločno prekratek.
%Dober povzetek vključuje: (1) kratek opis obravnavanega problema, (2) kratek opis vašega pristopa za reševanje tega problema in (3) (najbolj uspešen) rezultat ali prispevek diplomske naloge.
V diplomskem delu obravnavamo spodbujevano učenje v impulznih nevronskih mrežah, ki se obnašajo podobno kot človeški možgani. Predstavimo in rešimo ključne izzive učenja impulznih mrež ter razvijemo rešitve, ki upoštevajo tako biološko smiselnost kot zahtevnost simulacije. S prilagoditvijo klasične oblike sinapse, odvisne od nagrajevanja in časovne razporeditve impulzov (\textit{R-STDP}), omogočimo učinkovito učenje in dodeljevanje zaslug preteklim odločitvam, pri čemer ohranimo osnovni princip R-STDP, kjer sinapse kodirajo vpliv pretekle aktivnosti nevronov na izbrane akcije brez uvedbe negativnih nagrad ali nerealističnih mehanizmov.

Razviti sistem razširimo v arhitekturo akter–kritik, ki omogoča reševanje problemov z zakasnjenimi nagradami s prenosom pričakovane nagrade iz cilja v prejšnja stanja. Sistem ovrednotimo na poenostavljenih in nato na kompleksnejših nalogah, kot sta igra Pong in problem mrežnega sveta (angl. \textit{gridworld}). V igri Pong rezultati kažejo postopno daljše sekvence igranja brez zgrešitve žogice, v nalogi mrežnega sveta pa postopno izboljševanje strategije in krajšanje poti do cilja.

\bigskip

\noindent\textbf{Ključne besede:} \tkeywords.
% prazna stran
\clearemptydoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% abstract
\selectlanguage{english}
\addcontentsline{toc}{chapter}{Abstract}
\chapter*{Abstract}

\noindent\textbf{Title:} \ttitleEn
\bigskip

\noindent\textbf{Author:} \tauthor
\bigskip

%\noindent\textbf{Abstract:} 
\noindent %This sample document presents an approach to typesetting your BSc thesis using \LaTeX. 
%A proper abstract should contain around 100 words which makes this one way too short.
In this thesis, we study reinforcement learning in spiking neural networks, a type of neural network that behaves similarly to the human brain. We present and address key challenges in training spiking networks and develop solutions that account for both biological plausibility and simulation complexity. By adapting the classical form of reward modulated spike timing dependent plasticity (\textit{R-STDP}), we enable efficient learning and the assignment of credit to past decisions, while preserving the core principle of R-STDP, in which synapses encode the influence of past neuronal activity on selected actions, without introducing negative rewards or unrealistic mechanisms.

We then extend the developed system into an actor–critic architecture, enabling it to solve problems with delayed rewards by propagating the expected reward from the goal back to previous states. The system is first evaluated on simplified tasks and then on more complex problems, such as the game Pong and the gridworld problem. In Pong, the results show progressively longer sequences of play without missing the ball, while in the gridworld task the strategy gradually improves and the path to the goal becomes shorter.
\bigskip

\noindent\textbf{Keywords:} \tkeywordsEn.
\selectlanguage{slovene}
% prazna stran
\clearemptydoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\mainmatter
\setcounter{page}{1}
\pagestyle{fancy}

\chapter{Uvod}
\label{intro}
Impulzne nevronske mreže predstavljajo razred nevronskih mrež, katerih delovanje temelji na diskretnih impulzih in izraziti časovni dinamiki. Za razliko od klasičnih umetnih nevronskih mrež, ki informacije obdelujejo z zveznimi aktivacijami v diskretnih slojih, impulzne nevronske mreže temeljijo na asinhronih dogodkih v času, kar jih približa dejanskemu delovanju bioloških možganov. Njihova ključna prednost ni le v potencialni računski učinkovitosti, temveč predvsem v možnosti modeliranja znanih bioloških mehanizmov, kot so časovna integracija signalov, sinaptična plastičnost in nevromodulacija.

Raziskovanje impulznih nevronskih mrež je smiselno tako z vidika umetne inteligence kot tudi računske nevroznanosti. Takšni modeli omogočajo preučevanje, kako lahko iz lokalnih pravil učenja, osnovanih na aktivnosti posameznih nevronov in sinaps, ter globalnih nagradnih signalov vznikne smiselno vedenje. S tem se umetna inteligenca približa razumevanju učenja v vedenjskem smislu, kjer sistem ne optimizira vnaprej znane funkcije, temveč se skozi interakcijo z okoljem postopoma prilagaja in oblikuje strategije delovanja.

Posebej pomemben okvir za takšno učenje predstavlja spodbujevano učenje, ki je v bioloških sistemih tesno povezano z delovanjem dopaminskega sistema. Medtem ko so algoritmi spodbujevanega učenja v klasičnem strojnem učenju dobro uveljavljeni, njihova neposredna uporaba v impulznih nevronskih mrežah ni mogoča zaradi drugačne narave signalov, izrazite časovne odvisnosti in zahtev po biološki verjetnosti. To odpira vprašanje, kako zasnovati učne mehanizme, ki so hkrati učinkoviti pri reševanju nalog in skladni z znanimi procesi v možganih.

Osrednji problem, ki ga obravnavamo v tem delu, je učenje v impulznih nevronskih mrežah v okoljih s takojšnjimi in zakasnjenimi nagradami. Enostavnejši mehanizmi, kot je sinaptična plastičnost, odvisna od nagrajevanja in časovne razporeditve impulzov (angl. \textit{reward modulated spike timing dependent plasticity}, \textit{R-STDP}), omogočajo učenje v primerih, kjer nagrada sledi akciji neposredno, vendar odpovejo pri nalogah, kjer je nagrada časovno oddaljena. S tem se pojavi problem časovne dodelitve zaslug (angl. \textit{credit assignment}), ki predstavlja enega ključnih izzivov spodbujevanega učenja v biološko realističnih modelih.

V tej diplomski nalogi se tega problema lotimo postopno. Najprej v poglavju \ref{neuron_synapse_modelling} predstavimo in ovrednotimo različne modele nevronov in sinaps, ki služijo kot temelj za nadaljnji razvoj učnih mehanizmov. Na tej osnovi v poglavju \ref{sec:rstdp} razvijemo sistem spodbujevanega učenja, ki temelji izključno na impulznih nevronskih mrežah. S prilagoditvijo klasične sinapse R-STDP omogočimo učinkovitejše dodeljevanje zaslug preteklim odločitvam, pri tem pa ohranimo osnovni princip lokalnega učenja. Pomembna značilnost pristopa je uporaba izključno nenegativnih nagradnih signalov, saj v bioloških dopaminskih sistemih negativni dopamin ne obstaja; znižanje dopaminske aktivnosti predstavlja odsotnost ali zmanjšanje pričakovane nagrade, ne pa ločen negativni signal. To razlikuje naš pristop od številnih obstoječih metod, ki uporabljajo eksplicitne negativne nagrade.

Ker opisani pristop še vedno ne omogoča učinkovitega reševanja nalog z daljšo časovno odvisnostjo nagrad, v poglavju \ref{sec:td_learning} razviti sistem razširimo z učenjem na podlagi časovne razlike (angl. \textit{temporal-difference learning}) v impulzno nevronsko arhitekturo akter-kritik, navdihnjeno z dopaminskimi vezji bazalnih ganglijev. V takšnem sistemu akter skrbi za izbiro akcij, medtem ko kritik ocenjuje pričakovano prihodnjo nagrado posameznih stanj in generira učni signal za prilagajanje sinaps. Ta razširitev omogoča propagacijo pričakovane nagrade iz ciljnih stanj nazaj v prejšnja stanja ter s tem učinkovito učenje v okoljih z zakasnjenimi nagradami. Delovanje razširjenega sistema ovrednotimo na problemu mrežnega sveta (angl. \textit{gridworld}), kjer se sistem postopno uči boljših strategij in krajših poti do cilja.

V ta namen v poglavju \ref{neuron_synapse_modelling} najprej predstavimo in ovrednotimo različne modele nevronov in sinaps ter njihove lastnosti. Nato v poglavju \ref{sec:rstdp} razvijemo sistem, ki temelji na prilagojeni sinapsi R-STDP in omogoča učinkovitejše dodeljevanje zaslug preteklim odločitvam v nalogah s takojšnjimi nagradami, kar prikažemo na nalogi igranja igre \href{https://en.wikipedia.org/wiki/Pong}{Pong}. Ker takšen pristop še ne omogoča učenja v okoljih z daljšo časovno odvisnostjo nagrad, v poglavju \ref{sec:td_learning} sistem razširimo z učenjem na podlagi časovne razlike (\textit{TD} učenje) v impulzno nevronsko arhitekturo akter–kritik, navdihnjeno z dopaminskimi vezji bazalnih ganglijev. Razširjeni sistem omogoča propagacijo pričakovane nagrade v prejšnja stanja in je ovrednoten na problemu mrežnega sveta, kjer se agent postopno uči učinkovitejše strategije in krajše poti do cilja.

\chapter{Pregled področja in sorodnih del}
Raziskave impulznih nevronskih mrež so se v zadnjih desetletjih razvijale predvsem na presečišču umetne inteligence, nevroznanosti in nevromorfnega inženirstva. Osrednji izziv na tem področju predstavlja učenje, saj klasični gradientni pristopi, ki se uporabljajo pri umetnih nevronskih mrežah, niso neposredno uporabni zaradi diskretne narave impulzov in nelinearne časovne dinamike.

Eden temeljnih pristopov k učenju v impulznih nevronskih mrežah je sinaptična plastičnost, odvisna od časovne razlike med impulzi pre- in postsinaptičnih nevronov (angl. \textit{spike timing dependent plasticity}, \textit{STDP}). Razširitve tega mehanizma z globalnim nagradnim signalom, kot je R-STDP, omogočajo uporabo impulznih nevronskih mrež v okviru spodbujevanega učenja. Takšni pristopi so uspešni pri nalogah s takojšnjimi nagradami, vendar se soočajo z omejitvami pri reševanju problemov z zakasnjenimi nagradami, kjer je potrebna časovna propagacija učnega signala.

Na področju spodbujevanega učenja obstaja obsežna literatura, ki obravnava različne naloge, kot so navigacija, vodenje robotov in igranje iger. Dela, kot je \cite{robot}, obravnavajo uporabo spodbujevanega učenja v robotskih sistemih, kjer agent uči strategije na podlagi interakcije z realnim, šumnim okoljem. Podobni pristopi so uporabljeni tudi v simuliranih okoljih, kjer se raziskujejo lastnosti učnih algoritmov, na primer pri problemu vozička s palico \cite{vozicekSPalico} ali pri igranju iger \cite{predvidevanjeAkcij}. V teh delih so uporabljeni predvsem klasični modeli nevronskih mrež ali simbolni opisi stanj, časovna dinamika pa ni eksplicitno modelirana na ravni posameznih dogodkov.

Impulzne nevronske mreže se od rekurentnih arhitektur, kot sta LSTM ali GRU, razlikujejo po tem, da čas ni implicitno kodiran v stanju mreže, temveč je neposredno prisoten v obliki časovnih zamikov med impulzi. To omogoča naravno obdelavo dogodkovno vodenih in asinhronih signalov, hkrati pa odpira možnost energetsko učinkovite implementacije na nevromorfni strojni opremi. Pri tem računanje ne temelji na zaporednem množenju matrik, temveč na redkih dogodkih, kjer se ob pojavu impulza posodobi le del mreže.

Posebno zanimiva smer raziskav je povezovanje impulznih nevronskih mrež z učenjem na podlagi časovne razlike. V delu \cite{pilotStudy} je predstavljena uporaba TD učenja na trdo-ožičeni (nevromorfni) impulzni nevronski mreži, kjer je končna naloga igranje igre Pong. Trdo-ožičena implementacija pomeni, da je mreža realizirana na specializirani strojni opremi, kjer so nevroni in sinapse fizično implementirani, kar omogoča visoko energetsko učinkovitost, hkrati pa omejuje fleksibilnost modela.

Biološko bolj neposredna implementacija TD učenja je arhitektura akter-kritik, ki je navdihnjena z delovanjem bazalnih ganglijev in dopaminskega sistema v možganih \cite{actorCritic}. V tem okviru kritik ocenjuje vrednost stanj in generira dopaminski signal, ki predstavlja napako napovedi nagrade, medtem ko akter na tej osnovi prilagaja strategijo izbire akcij. Takšen nagradni sistem je tesno povezan z eksperimentalnimi ugotovitvami o delovanju dopaminskih nevronov, ki kodirajo razliko med pričakovano in dejansko nagrado.

V primerjavi z obstoječimi deli se ta diplomska naloga osredotoča na enotno impulzno nevronsko arhitekturo, ki združuje lokalna pravila učenja, globalne nagradne signale in biološko smiselno implementacijo brez uporabe eksplicitnih negativnih nagrad. Poudarek je na časovni dodelitvi zaslug in postopni razširitvi osnovnega R-STDP pristopa v arhitekturo akter-kritik, ki omogoča učenje v okoljih z zakasnjenimi nagradami.

\chapter{Modeliranje nevronov in sinaps}
\label{neuron_synapse_modelling}
Impulzne nevronske mreže so določene z modelom nevrona in modelom sinapse, ki povezuje nevrone. Obstaja veliko modelov, v nadaljevanju pa bosta predstavljena in primerjana dva modela nevronov glede na njuno uporabnost pri spodbujevanem učenju na impulznih nevronskih mrežah. Predstavljen bo tudi model sinapse, primeren za spodbujevano učenje, ki bo uporabljen v sistemih, razvitih v nadaljevanju. 

Nevronski modeli opisujejo električne lastnosti celične membrane nevrona v možganih. Nevroni prek sinaps sprejemajo izhodne (postsinaptične) tokove nevronov, s katerimi so povezani, in skozi čas glede na utež sinapse posodabljajo svoj membranski potencial. Tok, ki prek sinapse s pozitivno utežjo od nevrona na začetku sinapse (presinaptičnega nevrona) prihaja do nevrona na koncu (postsinaptičnega nevrona), povzroči zvišanje membranskega potenciala. Ko membranski potencial nevrona preseže vrednost $V_{th}$, se sproži impulz, pri čemer ta nevron sprosti svoj postsinaptični tok na sinapso. Ta sistem je prikazan na sliki \ref{pre-sin-post}, kjer so predstavljeni postsinaptični tokovi presinaptičnih nevronov $i_{\text{syn}}(t - t_j - d_j)$. Indeks $j$ označuje posamezne presinaptične nevrone, $t$ predstavlja trenutni čas, $t_j$ čas sprožitve akcijskega potenciala presinaptičnega nevrona, $d_j$ pa zakasnitev signala zaradi prenosa prek sinapse do postsinaptičnega nevrona.

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=1.0\textwidth]{neuron_models/pre-sin-post.png}
\end{center}
\caption{Prikaz pre- in postsinaptičnih razmerij ter posameznih postsinaptičnih tokov.}
\label{pre-sin-post}
\end{figure}

\noindent Vrednost membranskega potenciala se ne glede na vhodne tokove s časom zmanjšuje zaradi uhajalske prevodnosti $g_L$. Takim nevronskim modelom pravimo tokovno gnani modeli uhajajočega integrirajočega nevrona (\textit{angl. leaky integrate-and-fire model} ali \textit{leaky IAF}). Po sprožitvi impulza se velikost postsinaptičnega toka spreminja po krivulji, ki jo določa izbrano jedro modela in predstavlja obliko toka po impulzu. V nadaljevanju bomo predstavili dva pristopa: enostavnejši model z eksponentno oblikovanim postsinaptičnim tokom in kompleksnejši model z alfa oblikovanimi postsinaptičnimi tokovi. Obe obliki sta pri enakih parametrih prikazani na sliki \ref{neuron_psc} in podrobneje, skupaj s parametri, opisani v nadaljevanju. Označenih je tudi $36,8 \%$ maksimalne vrednosti, ki jo, po izračunih v poglavju \ref{eksponentno_jedro}, eksponentno jedro doseže ravno pri $t=\tau_{\text{syn, ex}}$, času, ko alfa jedro doseže svojo maksimalno vrednost.
\newpage
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=1.0\textwidth]{neuron_models/PSCs}
\end{center}
\caption{Postsinaptični tok modela z eksponentnim in alfa jedrom pri sinaptični uteži $w=50$ ter $\tau_{\text{syn, ex}}=5~\mathrm{ms}$. Eksponentno jedro povzroči takojšen skok postsinaptičnega
toka ob sprožitvi impulza in eksponentno odtekanje, medtem ko alfa jedro modelira
postopen dvig toka do maksimuma in nato počasnejši upad, kar bolje odraža časovno dinamiko
bioloških sinaps.}
\label{neuron_psc}
\end{figure}
\noindent Membranski potencial leaky IAF nevrona (\textit{LIF nevron}) se spreminja glede na ravnovesje med kapacitivnostjo in uhajanjem prek membranske prevodnosti, vhodne tokove $I_{\text{syn}}$ ter zunanji šum $I_e$. Model membrane, ki določa, kako se spreminja membranski potencial, je definiran z naslednjimi parametri.

\begin{itemize}
    \item \textbf{$E_L$ --- mirujoči membranski potencial} \\
    Električni potencial, na katerega se membranski potencial relaksira v odsotnosti vhodnih tokov;

    \item \textbf{$C_m$ --- membranska kapacitivnost} \\
    Kapacitivnost membrane, ki določa, kako hitro se membranski potencial odziva na vhodne tokove;

    \item \textbf{$\tau_m$ --- membranska časovna konstanta} \\
    Čas, v katerem membrana pasivno integrira tok; definiran kot razmerje med kapacitivnostjo $C_m$ in uhajalsko prevodnostjo $g_L$ (\textit{leakage conductance}). Konstanto $\tau_m$ lahko definiramo tudi kot produkt med kapacitivnostjo in uporom membrane $\tau_m = C_m R_m = \frac{C_m}{g_L}$;

    \item \textbf{$t_{ref}$ --- refrakcijskio obdobje} \\
    Čas, v katerem se nevron po sprožitvi akcijskega potenciala ne more ponovno prožiti;

    \item \textbf{$V_{th}$ --- prag proženja} \\
    Membranski potencial, pri katerem nevron sproži akcijski potencial;

    \item \textbf{$V_{\min}$ --- spodnja meja membranskega potenciala} \\
    Absolutna spodnja meja za membranski potencial;

    \item \textbf{$I_e$ --- zunanji konstantni tok} \\
    Dodani tok, ki modelira stalni zunanji šum.
\end{itemize}
Membranski potencial $V_m$ pri tokovno gnanem modelu uhajajočega integrirajočega nevrona je glede na prej navedene parametre opisan z diferencialno enačbo.
\begin{equation*}
    \frac{dV_m}{dt}=-\frac{V_m-E_L}{\tau_m}+\frac{I_{\text{syn}}+I_e}{C_m}.
\end{equation*}
Prvi člen na desni strani enačbe opisuje pasivno uhajanje membranskega potenciala proti mirujoči vrednosti $E_L$ s časovno konstanto $\tau_m$, ki določa hitrost relaksacije membrane. Drugi člen predstavlja vpliv vhodnih tokov, kjer $I_{\text{syn}}$ označuje skupni sinaptični tok, ki ga ustvarijo vsi presinaptični nevroni, $I_e$ pa zunanji konstantni tok ali šum. Membranska kapacitivnost $C_m$ določa, kako močno posamezni tokovi vplivajo na spremembo membranskega potenciala. Skupni tok $I_{\text{syn}}$, ki ga postsinaptični nevron prejme prek vseh sinaps, lahko razdelimo na dve komponenti glede na vrednosti uteži sinaps. Če je utež sinapse pozitivna, bo membranski potencial glede na postsinaptični tok naraščal proti pragu proženja. Pravimo, da je takšna povezava vzbujajoča. Obratno, če je utež sinapse negativna, bo membranski potencial padal. Takšni povezavi pravimo inhibitorna povezava. Notacijo $I_{\text{syn}}$ in $i_{\text{syn}}$ bomo v nadaljevanju razdelili na $i_{\text{syn, ex}}$ in $i_{\text{syn, in}}$, skupno zapisano kot $i_{\text{syn, X}}$, kjer je $X \in \{\text{ex, in}\}$, $I_{\text{syn}}$ pa bomo zapisali kot vsoto $I_{\text{syn, ex}}$ in $I_{\text{syn, in}}$. Tak splošnejši zapis omogoča uporabo različnih sinaptičnih časovnih konstant, definiranih v nadaljevanju, za vzbujajoče in inhibitorne povezave. $I_{\text{syn}}$ tako zapišemo kot

\[
I_{\text{syn}}(t) = I_{\text{syn, ex}}(t) + I_{\text{syn, in}}(t) ,
\]

\noindent kjer

\[
I_{\text{syn, X}}(t) = \sum_j w_j \sum_k i_{\text{syn, X}}(t - t_j^k - d_j) ,
\]

\noindent kjer $j$ teče po vzbujajočih (X = ex) in inhibitornih (X = in) sinapsah z utežmi $w_j$ do presinaptičnih nevronov, $k$ pa po časih impulzov nevrona $j$. $d_j$ predstavlja zakasnitev zaradi potovanja signala po sinapsi do nevrona $j$. $i_{\text{syn, X}}(t - t_j^k - d_j)$ predstavlja postsinaptični tok nevrona $j$. \\ 
\\
Postsinaptični tokovi so ne glede na jedro določeni z naslednjima parametroma.

\begin{itemize}
    \item \textbf{$\tau_{\mathrm{syn,ex}}$ --- sinaptična časovna konstanta (vzbujajoča)} \\
    Čas, ki določa hitrost naraščanja postsinaptičnega toka po proženju. Pri modelu z alfa-jedrom predstavlja čas dviga alfa-funkcije; pri eksponentnem jedru pa čas padca eksponentne funkcije, pri kateri je čas dviga neskončno majhen;

    \item \textbf{$\tau_{\mathrm{syn,in}}$ --- sinaptična časovna konstanta (inhibitorna)} \\
    Čas, ki določa hitrost naraščanja postsinaptičnega toka po proženju, vendar za inhibitorne sinapse.
\end{itemize}

Opisana dinamika je prikazana na sliki \ref{mem_pot}, kjer je membranski potencial leaky IAF nevrona (\textit{LIF nevrona}) prikazan kot odziv na skupni presinaptični tok dveh nevronov: enega z vzbujajočo in enega z inhibitorno povezavo, oba z eksponentno oblikovanim postsinaptičnim tokom. Na grafu je razvidno, kako $\tau_m$ vpliva na hitrost spremembe membranskega potenciala ter na odtekanje potenciala proti mirujočemu potencialu $E_L$ po impulzu in med refrakcijskim obdobjem.

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=1.0\textwidth]{neuron_models/mem_pot.png}
\end{center}
\caption{Sprememba membranskega potenciala $V_m$ LIF nevrona (\textit{leaky IAF}) kot funkcija presinaptičnega toka $I_{\text{syn}}$, ki ga ustvarjata dva presinaptična nevrona z eksponentnim postsinaptičnim tokom: vzbujajoči nevron z utežjo $w_{\text{ex}}=6$ in inhibitorni nevron z utežjo $w_{\text{in}}=-6$. Ostali parametri so $\tau_{\text{syn, ex}} = \tau_{\text{syn, in}} = 5~\mathrm{ms}$, zakasnitev $d_j = 0~\mathrm{ms}$, $V_{\text{th}}=-50~\mathrm{mV}$, $E_L=-65~\mathrm{mV}$, $V_{\min}=-80~\mathrm{mV}$, $t_{\text{ref}}=5~\mathrm{ms}$, $I_e=0~\mathrm{nA}$ in $C_m=1~\mathrm{nF}$. Prikazana je krivulja za $\tau_m=10~\mathrm{ms}$ in $\tau_m=5~\mathrm{ms}$ za primerjavo vpliva časovne konstante membrane na dinamiko. Vzbujajoči nevron se sproži ob $t=5~\mathrm{ms}$, inhibitorni pa ob $t=35~\mathrm{ms}$.} 
\label{mem_pot}
\end{figure}

\newpage
\section{Model z eksponentnim jedrom}
\label{eksponentno_jedro}
V simulatorju NEST, ki ga bomo uporabili za implementacijo kasnejših sistemov, je model z eksponentnim jedrom (\textit{iaf\_psc\_exp}) definiran s sistemom diferencialnih enačb prvega reda \cite{expModel}. Postsinaptični tok $i(t)$ se spreminja po sistemu
\begin{align*}
    \frac{dx}{dt}&=\frac{z}{\tau_{rec}}-ux\delta({t-t_{sp}}) \\
    \frac{di}{dt}&=-\frac{i}{\tau_{\text{syn, X}}}+ux\delta({t-t_{sp}}) \\
    \frac{dz}{dt}&=\frac{i}{\tau_{\text{syn, X}}}-\frac{z}{\tau_{rec}} ,
\end{align*}
kjer $t_{sp}$ predstavlja čas presinaptičnega impulza, 
$\tau_{rec}$ čas povrnitve sinaptičnih virov,
$u$ delež sinaptičnih virov, porabljenih pri impulzu, in
$\delta(t-t_{sp})$ delta porazdelitev za instantne posodobitve ob impulzih.
\\
\\
Preverimo, ali postsinaptični tok po impulzu res sledi preprosti eksponentni funkciji. Če opazujemo samo spreminjanje $i(t)$ skozi čas brez novih impulzov, velja $\delta(t-t_{sp})=0$ in se diferencialna enačba za $i$ poenostavi v
\begin{equation*}
    \frac{di}{dt}=-\frac{i}{\tau_{\text{syn, X}}} .
\end{equation*}
Rešitev te diferencialne enačbe je tako
\begin{equation*}
    i(t)=i_0 e^{-t/\tau_{\text{syn, X}}} ,
\end{equation*}
kjer vidimo, da je jedro res eksponentna funkcija z začetkom v $i_0$. Skok potenciala po impulzu je določen z utežjo sinapse $w$, zato je $i_0=w$, postsinaptični tok pa je določen z vrednostjo $\tau_{\text{syn, X}}$.

Zdaj lahko izračunamo količino naboja, ki ga po impulzu prenesemo po sinapsi. Ta nam bo koristila pri primerjavi in izbiri ustreznega modela za sisteme, razvite v nadaljevanju (razdelek \ref{sec:izbira_modela}). Količina naboja $q$, ki se po sprožitvi impulza prenese prek sinapse, je definirana
kot ploščina pod krivuljo postsinaptičnega toka
\[
q = \int_0^{\infty} i_{\text{syn, X}}(t) dt = \tau_{\text{syn, X}} .
\]

Na sliki \ref{neuron_psc} je poleg parametra $\tau_{\text{syn, ex}}$ označenih tudi $36,8\%$ maksimalnega postsinaptičnega toka. Pri eksponentnem jedru bo postsinaptični tok to vrednost dosegel natanko pri $\tau_{\text{syn, ex}}$, kar smo izračunali po naslednji enačbi.
\begin{align*}
    i_{\text{syn, ex}}(t) &= w e^{-\frac{t}{\tau_{\text{syn, ex}}}} \\
    i_{\text{syn, ex}}(\tau_{\text{syn, ex}}) &= w e^{-1} \approx 0.3679 w.
\end{align*}

\section{Model z alfa jedrom}
Model z alfa jedrom je kompleksnejši in biološko bolj realističen model postsinaptičnih tokov. V simulatorju NEST je postsinaptični tok modela z 
alfa jedrom (\textit{iaf\_psc\_alpha}) definiran kot
\[
i_{\text{syn, X}}(t) = \frac{e}{\tau_{\text{syn, X}}} t e^{-\frac{t}{\tau_{\text{syn, X}}}} \Theta(t) ,
\]

kjer je $\Theta(t)$ enotina stopnica, ki zagotavlja, da je postsinaptični tok
ničeln za čase pred sprožitvijo impulza.
Postsinaptični tok je normaliziran tako, da doseže enotski maksimum ob času
$t = \tau_{\text{syn, X}}$.

\[
i_{\text{syn, X}}(t = \tau_{\text{syn, X}}) = 1 .
\]
Enačba opisuje časovni potek postsinaptičnega toka po sprožitvi
akcijskega potenciala presinaptičnega nevrona. Faktor $t$ povzroči postopen
dvig toka po impulzu, medtem ko eksponentni člen določa njegovo kasnejše
zmanjševanje. Posledično postsinaptični tok ne doseže maksimuma takoj ob
sprožitvi impulza, temveč šele ob času $\tau_{\text{syn, X}}$, kar modelira hitrost
odpiranja in zapiranja ionskih kanalov v bioloških sinapsah.

Parameter $\tau_{\text{syn, X}}$ določa časovno skalo dinamike sinapse: večja
vrednost povzroči počasnejši dvig in daljše trajanje postsinaptičnega toka,
manjša pa hitrejši in krajši odziv. Takšna oblika postsinaptičnega toka vodi v
bolj zglajeno časovno integracijo vhodnih impulzov.

Enako kot pri eksponentnem jedru skupni naboj $q$, ki ga prenese postsinaptični tok pri alfa jedru, definiramo kot ploščino pod krivuljo postsinaptičnega toka.
\[
q = \int_0^{\infty} i_{\text{syn, X}}(t) dt = e \tau_{\text{syn, X}} .
\]

\section{Izbira modela nevrona}
\label{sec:izbira_modela}
V sistemih, ki jih bomo implementirali v nadaljevanju, skušamo pri modeliranju mehanizmov v človeških možganih uporabiti čim manj poenostavitev ali posplošitev. Za to je bolj primeren model
nevrona z alfa jedrom, ki ima biološko bolj realistično obliko postsinaptičnega toka. V nadaljevanju sta kljub temu uporabljena oba modela, saj se zaradi različnih oblik postsinaptičnega toka za spodbujevano učenje bolje obnese model z eksponentnim jedrom.

Za nas je najpomembnejša razlika v količini prenesenega naboja $q$. Kot bo opisano v razdelku \ref{sec:rstdp}, to namreč vpliva na to, koliko lahko zunanji šum vpliva na frekvenco impulzov. Količina prenesenega naboja $q_{\text{alfa}}$ je pri alfa jedru večja od prenesenega naboja pri eksponentnem jedru $q_{\text{exp}}$ za faktor $\frac{q_{\text{alfa}}}{q_{\text{exp}}} = e$. To razliko lahko prilagodimo z nižjimi vrednostmi uteži sinaps, razlika v vplivu na frekvenco impulzov pa je posledica različno dolgega časovnega intervala, v katerem je postsinaptični tok blizu maksimalne vrednosti. Pri alfa jedru je ta interval večji kot pri eksponentnem jedru, zaradi česar bodo zaporedni postsinaptični impulzi skozi čas precej bolj prekrivni. Pri integriranju različnih postsinaptičnih tokov skozi čas tako pride do učinka nizkoprepustnega filtra, ki ublaži nenadne spremembe v amplitudi skupnega toka na vhodu v postsinaptični nevron. Če se nevron proži z določeno stalno frekvenco, bo ob dodanem šumu varianca v frekvenci impulzov pri alfa jedru manjša kot pri eksponentnem.
\\
\\
Primerjali bomo varianco frekvence pri obeh jedrih v času 5000 ms prek petih postsinaptičnih nevronov, v katere neodvisno injiciramo Poissonov šum. 

\subsubsection{Poissonov šum}
Poissonov šum formalno opišemo kot realizacijo Poissonovega procesa, ki je stohastični proces z diskretnimi dogodki v času. Poissonov proces opisuje zaporedje impulzov, ki se pojavijo neodvisno drug od drugega s konstantno povprečno stopnjo pojavljanja. Če Poissonov proces opazujemo v končnem časovnem intervalu $\Delta t$, je število impulzov v tem intervalu porazdeljeno po Poissonovi porazdelitvi.

Naj bo $N(t)$ Poissonov proces s parametrom $\lambda$, kjer $N(t)$ označuje skupno število impulzov do časa $t$. Tedaj velja, da je verjetnost, da se v intervalu $\Delta t$ pojavi $k$ impulzov, dana z izrazom
\begin{equation*}
    P(N(t + \Delta t) - N(t) = k) = \frac{(\lambda \Delta t)^k e^{-\lambda \Delta t}}{k!}, \quad k = 0, 1, 2, \ldots
\end{equation*}
Poissonov šum v impulznih nevronskih mrežah predstavlja vzorčenje iz Poissonovega procesa, kjer vsak impulz ustreza enemu akcijskemu potencialu. Takšen model ustreza biološki realnosti, saj impulzi v mnogih nevronih nastajajo približno neodvisno in z variabilnimi medimpulznimi intervali. Parameter $\lambda$ določa povprečno frekvenco proženja, medtem ko stohastičnost Poissonovega procesa povzroča varianco v številu impulzov, ki je enaka pričakovani vrednosti, torej $E[N(\Delta t)] = Var(N(\Delta t)) = \lambda \Delta t$.

Z injiciranjem Poissonovega šuma v postsinaptične nevrone simuliramo realizacije Poissonovega procesa, ki delujejo kot naključni sinaptični vhodi. To omogoča analizo odziva impulzne nevronske mreže na biološko verodostojen stohastični dražljaj ter primerjavo stabilnosti in variance frekvence proženja različnih jeder.
\newpage
\noindent Da dosežemo čim bolj enako osnovno frekvenco impulzov postsinaptičnih nevronov pri jedrih, ki jih bomo primerjali, je utež sinapse med nevroni z alfa jedrom $w_{\text{alfa}}$ za faktor $e$ manjša od uteži sinaps do nevronov z eksponentnim jedrom $w_{\text{eksp}}$. Vsi parametri simulacije so navedeni v tabeli \ref{tab:variance_simulation_parameters}. Iz rezultatov simulacije, prikazanih v tabeli \ref{tab:isi_summary}, pričakovano opazimo večjo varianco pri ek\-sponent\-nem jedru.

\begin{table}[htbp]
\centering
\label{tab:variance_simulation_parameters}
\begin{tabular}{ll}
\hline
\textbf{Parameter} & \textbf{Vrednost} \\
\hline
Število postsinaptičnih nevronov & 5 \\
Trajanje simulacije & 5000 ms \\
$C_m$ & 250.0 pF \\
$\tau_m$ & 20.0 ms \\
$E_L$ & 0.0 mV \\
$V_\text{th}$ & 20.0 mV \\
$V_\text{reset}$ & 0.0 mV \\
$t_\text{ref}$ & 2.0 ms \\
$\tau_\text{syn,ex}$ & 5.0 ms \\
$w_{\text{eksp}}$ & 25.0 \\
$w_{\text{alfa}}$ & 25.0 / $e \approx 9.20$ \\
Frekvenca Poissonovega šuma & 8000 Hz na nevron \\
\hline
\end{tabular}
\caption{Parametri simulacije uporabljeni pri primerjavi modelov nevronov.}
\end{table}

\begin{table}[htbp]
\centering
\label{tab:isi_summary}
\begin{tabular}{lcc}
\hline
Jedro & Povprečje (ms) & Varianca (ms$^2$) \\
\hline
Eksponentno & \(7.846 \pm 0.021\) & \(\mathbf{0.402 \pm 0.028} \) \\
Alfa       & \(7.800 \pm 0.023\) & \(\mathbf{0.270 \pm 0.006} \) \\
\hline
\end{tabular}
\caption{Povzetek statistike medimpulznih intervalov nevronov z alfa in eksponentnim jedrom. Povprečje in standardni odklon sta izračunana na vseh postsinaptičnih nevronih.}
\end{table}
\newpage
\section{R-STDP Sinaptični model}
\label{sec:synapse_model}
V sistemih, ki jih bomo implementirali v tej nalogi, bomo uporabljali sinapso s plastičnostjo, odvisno od nagrade in časovne razporeditve impulzov (\textit{angl. reward modulated spike timing dependent plasticity} ali \textit{R-STDP}).
Časovna razporeditev impulzov (\textit{STDP}) prilagaja sinaptične uteži glede na relativni čas impulzov pre- in postsinaptičnih nevronov. V svoji klasični obliki STDP uresničuje \href{https://en.wikipedia.org/wiki/Hebbian_theory}{Hebbov princip}:
\begin{quote}
``Nevroni, ki se skupaj prožijo, se povežejo.''
\end{quote}
Za spremljanje ``skupnega proženja'' parov nevronov bomo definirali vrednost STDP, ki predstavlja velikost potencialne posodobitve uteži sinapse med parom nevronov glede na čas impulza pre- in postsinaptičnega nevrona. V impulznih nevronskih mrežah to omogoča označevanje ali krepitev sinaps parov nevronov, ki so bili odgovorni za proženje izhodnih nevronov ob stimulaciji določenih vhodnih nevronov. Temu pravimo tudi dodeljevanje zaslug (\textit{credit assignment}), ki je eden temeljnih izzivov pri učenju impulznih nevronskih mrež. Pri klasičnih nevronskih mrežah bi v ta namen uporabili gradient, pri impulznih nevronskih mrežah pa to spremlja vrednost STDP, ki je pozitivna, če se presinaptični nevron sproži pred postsinaptičnim (\(\Delta t > 0\)), in negativna, če se presinaptični nevron sproži po postsinaptičnem (\(\Delta t \leq 0\)). Tako STDP služi kot groba aproksimacija gradienta. Matematično je to opisano s funkcijo okna STDP: 
\[
\mathrm{STDP}(\Delta t) =
\begin{cases}
A_+ e^{-|\Delta t|/\tau_+}, & \text{če } \Delta t > 0 \text{ (presinaptični pred postsinaptičnim)} \\
A_- e^{-|\Delta t|/\tau_-}, & \text{če } \Delta t \le 0 \text{ (postsinaptični pred presinaptičnim)}
\end{cases}
\]
\noident kjer sta \(A_+\) in \(A_-\) multiplikatorja za potenciranje in depresijo, \(\tau_+\) in \(\tau_-\) pa časovni konstanti, ki določata okno vpliva časovnih razlik.

Pare nevronov lahko posodabljamo izključno na podlagi vrednosti $\mathrm{STDP}$, s katero krepimo ali slabimo posamezne sinapse, običajno pa to vrednost uporabljamo v kombinaciji z nagrado. Količina nagrade oziroma koncentracija nevromodulatorja za celotno impulzno nevronsko mrežo ali skupino nevronov modulira amplitudo učenja oziroma posodabljanja povezav. Povezavo med parom nevronov bomo tako posodobili glede na vrednost STDP in prisotno količino nagrade v danem trenutku. 

\subsection{Dopaminska modulacija}
V vseh sistemih, razvitih v tej diplomski nalogi, kot osnovo za sinapso, ki poleg vrednosti STDP upošteva tudi nagrado, uporabljamo R-STDP model sinapse (\cite{distalReward}), kjer odvisnost od nagrade vpeljemo prek nevromodulatorja dopamina. Pogosto se koncentracija dopamina obravnava kot neposreden odraz količine nagrade oziroma vrednosti stanja, v katerem se agent nahaja, vendar ta modulira le plastičnost sinaps oziroma učenje. Namesto realizacije negativne nagrade z negativno koncentracijo dopamina, kar je pogost pristop, je biološko smiselneje izogibanje neželenim stanjem doseči z učenjem akcij, ki vodijo stran od teh stanj, ob pozitivni koncentraciji dopamina. Tak sistem je opisan v poglavju \ref{aversive}. 

Pri sinapsi R-STDP vrednost STDP spremljamo prek sledi upravičenosti \emph{eligibility trace} \(c\), ki se za posamezen par nevronov ob proženju zviša glede na vrednost STDP, nato pa skozi čas odteka glede na parameter $\tau_c$. To omogoča pripisovanje odgovornosti tudi ob zakasnjenih nagradah, glede na razliko med časom aktivnosti para nevronov in časom dovedene nagrade. Tako bodo sinapse, ki so bile aktivne tik pred zvišanjem koncentracije dopamina, obravnavane (glede na vrednost STDP) kot odgovorne za aktivnost izhodnih nevronov, ki je potencialno povzročila zvišanje koncentracije dopamina oziroma nagrado. Sinapse, aktivne dlje v preteklosti, bodo ustrezno manj posodobljene. Takemu učenju pravimo učenje s tremi dejavniki (\textit{three-factor learning}), ki označi sinapse za potencialno spremembo in ohranja informacijo o pretekli aktivnosti pre- (prvi faktor) in postsinaptičnega nevrona (drugi faktor), dokler ne prispe tretji faktor, v tem primeru dopaminski signal \(n\), ki predstavlja koncentracijo dopamina. Ta mehanizem olajša časovno dodelitev zaslug (\textit{temporal credit assignment}), saj omogoča povezovanje hitre aktivnosti nevronov s počasnejšimi vedenjskimi odzivi, s čimer sinapse prejmejo ustrezno posodobitev, tudi če je nagrada ali signal zakasnjen.

V primerjavi s klasičnimi časovno razlikovalnimi ali TD (angl. \textit{temporal difference}) algoritmi v ne-impulznih nevronskih mrežah, kjer se sledi upravičenosti uporabljajo za posodobitev stanj ali akcij v diskretnih časovnih korakih, je v spiking neural networks sled \(c\) neposredno vezana na dejanske sprožitve impulzov in se lahko sproži ob vsaki interakciji med pre- in postsinaptičnim nevronom. Tako sled upravičenosti predstavlja lokalno, biološko interpretabilno komponento učenja, ki je skladna z eksperimentalnimi dokazi o treh faktorjih učenja v možganih.

Dopaminsko koncentracijo predstavlja dopaminska sled \(n\), ki jo lahko definiramo kot vrednost, ki spremlja aktivnost posebnih dopaminergičnih nevronov. Ob zvišani aktivnosti dopaminergičnih nevronov se bo dopaminska sled zvišala, ob odsotnosti aktivnosti pa bo odtekala glede na parameter $\tau_n$. Dopaminska sled se uporablja za neposredno modulacijo velikosti in smeri sinaptične plastičnosti, tj. velikosti in predznaka posodobitve uteži povezave. Sinaptična dinamika sinapse R-STDP je opisana z enačbami po \cite{dopamineSynapse}:
\[
\begin{aligned}
\dot{w} &= c \, (n - b), \\
\dot{c} &= -\frac{c}{\tau_c} + \mathrm{STDP}(\Delta t) \, \delta(t - s_{\text{pre/post}}) C_1, \\
\dot{n} &= -\frac{n}{\tau_n} + \frac{\delta(t - s_n)}{\tau_n} C_2.
\end{aligned}
\]
Enačbe opisujejo dinamiko treh ključnih količin: sinaptične uteži \(w\), sledi upravičenosti \(c\) in dopaminske sledi \(n\). Sinaptična utež \(w\) se neposredno posodablja kot produkt trenutne sledi upravičenosti \(c\) in odstopanja dopaminske koncentracije \(n\) od bazalne vrednosti \(b\). Sled \(c\) spremlja pare sproženih pre- in postsinaptičnih nevronov ter aproksimira odgovornost posamezne sinapse za proženje postsinaptičnega nevrona kot posledico aktivnosti presinaptičnega nevrona. Njena dinamika vključuje eksponentno odtekanje s časovno konstanto \(\tau_c\) ter impulzne spremembe ob vsakem sproženju nevronov, modulirane s konstanto \(C_1\). Dopaminska sled \(n\) se eksponentno zmanjšuje z lastno časovno konstanto \(\tau_n\) in se poveča ob sprožitvah dopaminskih nevronov, ki se pojavijo ob časih \(s_n\) in so skalirane s konstanto \(C_2\). Slika \ref{c_no_delay} prikazuje spreminjanje sledi upravičenosti $c$ in uteži sinapse $w$ v odvisnosti od proženja pre- in postsinaptičnega nevrona ter dopaminske sledi $n$. Dopaminski nevroni, ki določajo dopaminsko sled, se prožijo na 40 ms.
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=0.9\textwidth]{neuron_models/RSTDP}
\end{center}
\caption{Sled upravičenosti $c$, dopaminska sled $n$ in spreminjanje sinaptične uteži pri presinaptičnih impulzih pri $[10.0, 30.0]$ ms ter postsinaptičnih impulzih pri $[12.0, 32.0]$ ms, simulirane v času $150$ ms pri sinapsi R-STDP s parametri $\tau_c = 50.0$ ms, $\tau_n = 10.0$ ms, $\tau_\mathrm{plus} = 10.0$ ms, $b = 0.0$, $A_\mathrm{plus} = 0.2$, $A_\mathrm{minus} = 0.2$ in sinaptično zakasnitvijo $0.5$ ms. S slik je razvidna pozitivna sled upravičenosti zaradi proženja presinaptičnega nevrona pred postsinaptičnim. Ob prisotnosti neničelne dopaminske sledi $n$ ob proženju dopaminergičnega nevrona vsakih 40 ms, se sinaptična utež $w$ okrepi.}
\label{c_no_delay}
\end{figure}

\chapter{Spodbujevano učenje na impulznih nevronskih mrežah}
\label{sec:reinforcement_learning}
V delu uporabljamo klasičnega agenta spodbujevanega učenja, ki prejema informacije o zunanjem okolju prek stimulacije vhodnih nevronov, nato pa kot odziv na trenutno stanje izbere akcijo, ki vpliva na okolje. Če se znajde v nagrajenem stanju, agenta nagradimo. S pomočjo nagrajevanja in interakcije z okoljem se agent nauči akcij, ki v določenem stanju privedejo do nagrade prek posodabljanja povezav, ki so bile aktivne v prejšnjem stanju in so bile odgovorne za akcijo, ki nas je pripeljala v naslednje stanje. 

\section{R-STDP učenje}
\label{sec:rstdp}
R-STDP učenje temelji na krepitvi povezav, ki so bile odgovorne za pravilno akcijo agenta v določenem stanju. To dosežemo tako, da prek vseh povezav zvišamo koncentracijo dopamina, pri čemer se najbolj okrepijo tiste povezave, ki so povzročile največ impulzov postsinaptičnih nevronov kot neposredna posledica proženja presinaptičnih nevronov. Take povezave imajo ob času prihoda nagrade najvišjo vrednost sledi upravičenosti. 

Naš agent je za začetek sestavljen iz $N_{in}$ vhodnih nevronov, ki predstavljajo možna stanja in so povezani z $N_a$ nevroni na izhodu. Vhod in izhod sta povezana po režimu \textit{all-to-all}, kjer so vsi vhodni nevroni povezani z izhodnimi nevroni. Ob prihodu v določeno stanje ustrezni vhodni nevron stimuliramo tako, da se ta za čas $200$ ms proži s frekvenco $100$ Hz. Akcijo izberemo na koncu intervala glede na aktivnost izhodnih nevronov, ki predstavljajo možne akcije. Med njimi izberemo nevron, ki se je v trenutnem stanju največkrat prožil. Če vstopimo v nagrajeno stanje, bomo $N_{\text{dopa}}$ dopaminskih nevronov stimulirali s tokom $600$ pA. Dopaminski nevroni ob impulzu enakomerno projicirajo dopamin med vse povezave med vhodnimi in izhodnimi nevroni.

Pri zvišani koncentraciji dopamina ob prihodu v nagrajeno stanje bi lahko poleg želenih povezav posodabljali tudi povezave, ki so aktivne v novem stanju. Da se temu izognemo, bomo onemogočili posodabljanje povezav, za katere je nagrada prišla prehitro. Sinapso R-STDP bomo zato prilagodili tako, da bomo celotno sled upravičenosti v času premaknili za vrednost $\tau_{c, \text{delay}}$ in s tem onemogočili posodabljanje zaradi nagrad, ki so prispele v času, krajšem od $\tau_{c, \text{delay}}$ po aktivnosti sinapse. Dinamika prilagojene sinapse je prikazana na sliki \ref{delayed_eligibility}. 

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=1.0\textwidth]{neuron_models/RSTDP_delayed}
\end{center}
\caption{Sled upravičenosti $c$, dopaminska sled $n$ in posodabljanje sinaptične uteži pri presinaptičnih impulzih pri $[10.0, 30.0]$ ms in postsinaptičnih impulzih pri $[12.0, 32.0]$ ms, simulirani v času $150$ ms pri sinapsi R-STDP s $\tau_c = 50.0$ ms, $\tau_{c,\mathrm{delay}} = 50.0$ ms, $\tau_n = 10.0$ ms, $\tau_\mathrm{plus} = 10.0$ ms, $b = 0.0$, $A_\mathrm{plus} = 0.2$, $A_\mathrm{minus} = 0.2$ in sinaptično zakasnitvijo $0.5$ ms. S slike je razvidna pozitivna sled upravičenosti zaradi proženja presinaptičnega nevrona pred postsinaptičnim, vendar zamaknjena za $\tau_{c, \mathrm{delay}}=50$ ms. Ob prisotnosti neničelne dopaminske sledi $n$ ob proženju dopaminergičnega nevrona vsakih 40 ms se sinaptična utež $w$ okrepi, vendar šele po zakasnitvi glede na impulze pre- in postsinaptičnega nevrona. Tako nagrada, ki prispe prehitro, ne povzroči okrepitve sinapse.}
\label{delayed_eligibility}
\end{figure}

Nagrada, ki jo določa aktivnost dopaminskih nevronov, bo vedno večja ali enaka $0$, kar pomeni, da se bodo sinapse s časom le krepile. Povezave, odgovorne za izbiro določene akcije v določenem stanju, morajo zato med seboj tekmovati za prevlado. Ob prisotnosti nagrade moramo povezave, ki so odgovorne za izbiro pravilne akcije, okrepiti dovolj v primerjavi z ostalimi, da bodo v prihodnje prevladale nad drugimi akcijami in povečale verjetnost izbire pravilne akcije. Razlika v amplitudi posodobitev posameznih povezav je v našem primeru, kjer je koncentracija dopamina za vse povezave v danem trenutku enaka, določena izključno s sledmi upravičenosti. 

Sled upravičenosti bo višja za povezave z višjimi utežmi, saj take povezave povzročajo več impulzov postsinaptičnega nevrona kot neposredna posledica proženja presinaptičnih nevronov. Pri nižjih vrednostih uteži, ko so vse povezave približno enake, pa dovolj veliko razliko v sledi upravičenosti dosežemo prek variance v frekvenci impulzov. Ta povzroči, da nekatere sinapse po naključju prispevajo k akciji nekoliko bolj ali manj. To pomeni, da se vrednosti sledi upravičenosti razlikujejo med sinapsami, posledično pa lahko dopaminska modulacija posodobi sinapse različno in vzpostavi razlikovanje med njimi, tudi če imajo vse povezave enake začetne uteži. Pri nižjih vrednostih uteži, kjer deterministični prispevek sinaps še ni dominanten, naključna varianca impulzov služi kot mehanizem, ki ``razbije simetrijo'' in omogoči, da se sledi upravičenosti razlikujejo, dokler uteži niso dovolj velike, da prevladujejo deterministični prispevki. 

Pogosto se razlikovanje med sinapsami doseže z dopuščanjem negativnih nagrad, ki sinapse, odgovorne za napačno izbrano akcijo, negativno posodobijo. V našem sistemu negativnih nagrad, ki zahtevajo negativno koncentracijo dopamina, ne bomo dopuščali, saj v naravi negativna koncentracija dopamina ni mogoča. Varianco v frekvenci impulzov bomo tako dosegli s Poissonovim šumom. V diferencialni enačbi, ki določa spreminjanje $V_m$, pri nižjih vrednostih uteži nad $I_{\text{syn}}$ prevladuje zunanji konstantni tok $I_e$, ki ga pri nas povzroča generator Poissonovega šuma. Tako ima, dokler ne prevladuje deterministični prispevek okrepljenih povezav, Poissonov šum večji vpliv na $V_m$ in je tako varianca v frekvenci impulzov večja. 

To služi tudi kot mehanizem za raziskovanje (angl. \textit{exploration}), ki bo pri nižjih utežeh, ko imajo vse povezave približno enake uteži, omogočil naključno izbiranje akcije. Tekom učenja, ko se sinapse okrepijo, bo agent akcije izbiral predvsem glede na prevladujoče sinapse, kar predstavlja izkoriščanje (angl. \textit{exploitation}) naučene politike (verjetnosti izbire posamezne akcije v določenem stanju). Naš sistem bo tako tekom učenja postopoma višal razmerje med izkoriščanjem in raziskovanjem.

Za model nevrona, ki ga bomo uporabili v nadaljevanju, bomo izbrali model nevrona z eksponentnim jedrom, saj Poissonov šum v tem primeru povzroči večjo varianco izhodnih nevronov kot model z alfa jedrom. To smo pokazali v poglavju \ref{sec:izbira_modela}. 

Glavni izziv za uspešno učenje je izbira pravilnih hiperparametrov sistema, ki omogočajo, da se sinapse diferencirajo dovolj hitro. Ker ne dopuščamo negativnih nagrad, bodo vse povezave s časom rasle, kar postopoma zmanjšuje verjetnost izbire naključne akcije. Tako se ob neustrezni izbiri parametrov lahko zgodi, da se vse povezave približno enakomerno krepijo, dokler raziskovanja praktično ni več in bo izbira akcije odvisna od naključno prevladujoče poti od vhodnih nevronov do izhodnih. Pravilne hiperparametre smo v nadaljevanju iskali eksperimentalno, pri čemer so najbolj vplivni hiperparametri frekvenca in utež povezave iz generatorja Poissonovega šuma do izhodnih nevronov, parametra $A_+$ in $A_-$ ter parameter $\tau_c$.

\subsection{Simulator NEST}
V nadaljevanju bomo vse razvite sisteme implementirali s pomočjo simulatorja \href{https://www.nest-simulator.org/}{NEST}. Zaledje in posamezne komponente simulatorja so implementirani v C++, kar omogoča učinkovito simulacijo impulznih nevronskih mrež. Za uporabo zakasnjene sinapse R-STDP, predstavljene v prejšnjih poglavjih, moramo za integracijo z ostalimi komponentami simulatorja napisati poseben C++ modul.
\\
\\
Za začetek bomo R-STDP učenje implementirali na preprosti nalogi s tremi stanji, oštevilčenimi z 0, 1 in 2. V vsakem stanju lahko agent izbere akcijo 0, 1 ali 2. Prehod v stanje po izbiri akcije je nakjučen (neodvisen od izbire akcije), v vsakem stanju pa je nagrajena le ena izbira akcije. V stanju 0, ki ga predstavlja stimulacija vhodnega nevrona 0, bomo kot nagrajeno izbrali akcijo 0, ki jo predstavlja izhodni nevron 0, v stanju 1 akcijo 1, v stanju 2 pa akcijo 2. Na sliki \ref{rstdp_simple} je razvidna prevlada pravilnih sinaps, višanje divergence v sinapsah skozi čas ter rast frekvence nagrad tekom učenja. V simulaciji uporabljamo privzete parametre NEST za nevrone z eksponentnim jedrom (\textit{iaf\_psc\_exp}) ter zakasnjene dopaminsko modulirane sinapse. Ostali parametri sistema so navedeni v tabeli \ref{tab:rstdp_simple}.
 
\begin{table}[htbp]
\label{tab:rstdp_simple}
\begin{tabular}{lll}
\textbf{Simbol} & \textbf{Pomen} & \textbf{Vrednost} \\ \hline
$t_{\text{SIM}}$ & Trajanje simulacije & 60000 ms \\ \hline
$w_{\text{motor, min}}$ & \makecell[l]{Minimalna utež sinaps \\ med vhodnimi in \\ izhodnimi nevroni} & 500 \\ \hline
$w_{\text{motor, max}}$ & \makecell[l]{Maksimalna utež sinaps \\ med vhodnimi in \\ izhodnimi nevroni} & 2000 \\ \hline
$\tau_c$ & & 5 ms \\ \hline
$\tau_{c,\mathrm{delay}}$ & & 200 ms \\ \hline
$\tau_n$ & Odtekanje dopaminske sledi & 10 ms \\ \hline
$\tau_+ = \tau_-$ & Pozitivna STDP konstanta & 20 ms \\ \hline
$b$ & \makecell[l]{Bazalna dopaminska \\ koncentracija} & 0.1 \\ \hline
$A_+$ & Pozitivni STDP multiplikator & 0.7 \\ \hline
$A_-$ & Negativni STDP multiplikator & 0.3 \\ \hline
$d$ & Zakasnitev sinaps & 0.5 ms \\ \hline
$\lambda_{\text{motor}}$ & \makecell[l]{Povprečna hitrost \\ Poissonovega šuma} & 1000 Hz \\ \hline
$w_{\text{Poisson}}$ & Uteži sinaps Poissonovega šuma & 100 \\  \hline
$w_{\text{init, motor}}$ & \makecell[l]{Začetne uteži sinaps \\ med vhodnimi in izhodnimi \\ nevroni} & $w_{\text{init, motor}} \sim \mathcal{N}(1300, 1)$ \\ \hline
\end{tabular}
\caption{Parametri simulacije R-STDP učenja na preprostem problemu.}
\end{table}

\begin{figure}[htbp]
\label{rstdp_simple}
\begin{center}
\includegraphics[width=1.0\textwidth]{rstdp/competition}
\end{center}
\caption{Prikaz uteži sinaps med vhodnimi in izhodnimi nevroni ter povprečne aktivnosti dopaminergičnih nevronov med simulacijo R-STDP učenja na preprostem problemu. Iz uteži je razvidno, da sinapse med vhodnimi in izhodnimi nevroni, ki predstavljajo pravilno akcijo, sčasoma prevladajo nad ostalimi akcijami. Posledično se frekvenca izbire pravilne akcije v posameznem stanju povečuje, kar je razvidno iz grafa povprečne aktivnosti dopaminergičnih nevronov, ki postaja vse ``gostejši''.} 
\label{competition}
\end{figure}
\newpage
\subsection{Igra Pong}
V nadaljevanju bomo R-STDP učenje predstavili na kompleksnejšem problemu: igri \href{https://en.wikipedia.org/wiki/Pong}{\textit{Pong}}. R-STDP učenje je kratkovidno, saj se naučimo akcij le, če nagrada sledi takoj, ne pa tudi, če je nagrada zakasnjena. Za zakasnjene nagrade lahko uporabimo TD (angl. \textit{Temporal Difference}) učenje, ki ga implementiramo v poglavju \ref{td_learning}, za zdaj pa bomo igranje igre Pong, ki sicer zahteva predvidevanje in ima zakasnjeno nagrado, preslikali na problem s takojšnjimi nagradami. Igro bomo definirali tako, da ima žogica stalno hitrost, določeno smer in pozicijo v $(x, y)$ ravnini. Na eni strani igrišča bo naš agent premikal platformo v horizontalni smeri, na drugi pa bo stena, od katere se žogica prožno odbije. Takšna konfiguracija je prikazana na sliki \ref{pong_setup}. Če bi od agenta zahtevali predvidevanje, bi morali stanja definirati kot kartezični produkt $(x, y)$ pozicije žogice, njene smeri in $x$ pozicije platforme. Problem bomo poenostavili v problem sledenja žogici, kot v delu \cite{pilotStudy}, kjer agent izbira želeno ciljno točko platforme. Predvidevanje zato ni potrebno. Tako stanja kot akcije agenta so diskretizirane možne $x$ pozicije žogice. Stanje je nagrajeno s stimulacijo dopaminskih nevronov s tokom $I_{R}$, ki je sorazmeren razliki med nagrado $R_b$, izračunani glede na oddaljenost želene pozicije $j$ od trenutne $x$ pozicije žogice $k$, in povprečno nagrado $\bar{R}_i$ v iteraciji $i$. S pomočjo povprečne nagrade omejimo krepitev sinaps, če te ne izboljšajo trenutne politike.

\begin{align*}
R_b &= \begin{cases}
    1-|j-k| \cdot 0.3 & \text{if} \ |j-k|\leq3, \\
    0 & \text{sicer}.
    \end{cases} \\
I_{R} &= \max(R_b-\bar{R}_i, 0) \cdot 600 \text{ pA}
\end{align*}

\begin{figure}[htbp]
\label{pong_setup}
\begin{center}
\includegraphics[width=0.5\textwidth]{figures/pong_setup}
\end{center}
\caption{Grafična predstavitev agenta in okolja (\cite{pilotStudy}).}
\end{figure}
\newpage
Pričakujemo, da bodo v posameznih stanjih prevladale sinapse, ki iz vhodnega nevrona vodijo do akcij okrog izhodnega nevrona, ki predstavlja isto $x$ pozicijo, kot jo ima v tistem trenutku žogica. Polje smo po $x$ osi diskretizirali na 20 stanj. Po simulaciji, ki je trajala 4000 iteracij (pozicij žogice) po 200 ms, na sliki \ref{rstdp_pong_learning} vidimo graf povezav med vhodnim nevronom, ki predstavlja pozicijo $x=5$, in 20 izhodnimi nevroni, kjer med učenjem prevladuje izhodni nevron 5, sledi pa mu izhodni nevron 4. Za simulacijo smo uporabili enake parametre kot pri primeru R-STDP učenja na preprostem problemu.

\begin{figure}[htbp]
\label{rstdp_pong_learning}
\begin{center}
\includegraphics[width=1.0\textwidth]{rstdp/rstdp-pong/learning}
\end{center}
\caption{Graf uteži povezav med vhodnim nevronom 5 in izhodnimi nevroni med simulacijo 4000 iteracij (pozicij žogice) po 200 ms igranja igre Pong. Med učenjem pričakovano prevladuje povezava do izhodnega nevrona 5, kar predstavlja sledenje žogici.}
\end{figure}
\newpage
\subsubsection{Rezultati}
Učenje spremljamo s povprečnim časom preživetja, ki predstavlja čas, ki je minil od zadnje zgrešitve žogice ali začetka igre. Povprečni čas preživetja je izračunan na podlagi zaporednih časov med posameznimi zgrešenimi odboji žogice. Ob vsaki iteraciji se čas od zadnjega zgrešenega udarca poveča za dolžino koraka simulacije (200 ms), ob zgrešenem udarcu pa se ponastavi na nič. Tako dobimo zaporedje časov preživetja, ki prikazuje, kako dolgo je agent uspešno odbijal žogico med dvema zaporednima napakama. Povprečni čas preživetja nato določimo kot kumulativno povprečje teh vrednosti skozi čas, kar pomeni, da pri vsaki časovni točki upoštevamo povprečje vseh do tedaj izmerjenih časov preživetja. Pričakujemo, da bo oblika krivulje skupne povprečne nagrade sledila krivulji povprečnega časa preživetja, saj bo agent ob uspešnem sledenju žogici prejemal višje in pogostejše nagrade. Pri skaliranih vrednostih povprečne nagrade in povprečnega časa preživetja lahko na sliki \ref{mean_reward} vidimo, da imata krivulji podobno obliko in da se skozi čas obe višata. Ker povprečna nagrada neposredno odraža izbiro pravilnih akcij v določenem stanju, lahko pri pravilno določeni funkciji nagrajevanja za poljuben problem kvaliteto učenja spremljamo zgolj s skupno povprečno nagrado.
\begin{figure}[htbp]
\label{mean_reward}
\begin{center}
\includegraphics[width=1.0\textwidth]{rstdp/rstdp-pong/mean_reward}
\end{center}
\caption{Skupna povprečna nagrada $\bar{R}_i$ in povprečen čas preživetja skozi 4000 iteracij po 200 ms. Pri usklajenem razponu obeh vrednosti je razvidno, da imata krivulji približno enako obliko in da obe skozi čas naraščata.}
\end{figure}
\\
\\
R-STDP učenje je v tej obliki učinkovito le pri nagradah, ki niso oddaljene, oziroma drugače povedano, agent se ne bo naučil potencialne poti skozi različna nenagrajena stanja, da bi prišel do končne nagrade. Primer problema z oddaljeno nagrado je iskanje poti do nagrade v mreži, kjer se agent lahko premika v sosednja polja levo, desno, gor in dol. Pri trenutni implementaciji se bo agent naučil prehoda le iz stanj, ki so neposredno ob nagrajenem stanju.
\\
Pri učenju bomo agenta nagradili, ko preide v končno stanje, nato pa ga postavili v naključno stanje. Končno politiko agenta bomo vizualizirali tako, da bomo v vsakem polju mreže $i$ prikazali preferirano smer. Verjetnost izbire posamezne akcije aproksimiramo s povprečno utežjo povezave od vhodnega do izhodnega nevrona. Preferirano smer za posamezno stanje $i$ bomo prikazali z normaliziranim vektorjem $\hat{x}_i$. Pričakujemo, da bodo vektorji stanj neposredno ob cilju kazali v smer cilja.
\[
\begin{aligned}
    \overrightarrow{x_i} &= \sum_{j=0}^{3} w_{ij} \cdot \overrightarrow{d}_j, \\
    L_i &= ||\overrightarrow{x_i}||, \\
    \hat{x}_i &= \begin{cases} \frac{\overrightarrow{x_i}}{L_i} & \text{if } L_i > 0 \\
        0 & \text{sicer}
    \end{cases},
\end{aligned}
\]

\noindent
kjer je \(w_{ij}\) utež sinapse iz vhodnega nevrona \(i\) do izhodnega nevrona \(j\) in $\overrightarrow{d}_j$ smerni vektor, ki predstavlja akcijo izhodnega nevrona \(j\):
\[
\overrightarrow{d}_0 = (0,1), \quad
\overrightarrow{d}_1 = (0,-1), \quad
\overrightarrow{d}_2 = (-1,0), \quad
\overrightarrow{d}_3 = (1,0).
\]

Indikator učenja pravilne akcije v stanju $i$ je tudi maksimalna razlika med utežmi med vhodnim nevronom $i$ in vsakim od izhodnih nevronov, kar predstavlja aproksimacijo verjetnosti izbire akcije. Pri dobri politiki želimo, da je verjetnost izbire pravilne akcije ustrezno večja od ostalih. Pri R-STDP učenju, kot smo ga implementirali doslej, pričakujemo opazno diferenciacijo akcij pri stanjih neposredno ob cilju, za ostala stanja pa pričakujemo, da bo maksimalna razlika med utežmi minimalna, saj v teh stanjih ne dovajamo nagrade, ki bi spodbujala učenje. Vektorji posameznih stanj in maksimalne razlike med utežmi so prek barve polj prikazani na sliki \ref{rstdp_3x3}. Slika prikazuje politiko po učenju 500 iteracij na $4 \times 4$ mreži in potrjuje pričakovane rezultate, kjer imajo veliko maksimalno razliko med utežmi in pravilno smer vektorjev le polja neposredno ob cilju. Rezultat potrjuje, da se agent ni sposoben naučiti poti do nagrade iz poljubnega stanja, temveč le iz stanj neposredno ob nagradi.

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=0.6\textwidth]{rstdp/rstdp-gridworld/best}
\end{center}
\caption{Prikaz politike modela na $4 \times 4$ mreži po 500 iteracijah po 200 ms. Končno stanje je obarvano zeleno. Barva ostalih stanj prikazuje maksimalno razliko med utežmi med vhodnim nevronom, ki predstavlja to stanje, in posameznimi izhodnimi nevroni, ki predstavljajo akcije. V vsakem polju je s puščico prikazan vektor preferirane akcije (smeri) glede na uteži med vhodnim nevronom in posameznimi izhodnimi nevroni. Vidimo, da so svetlo obarvana (imajo veliko maksimalno razliko med utežmi, ki predstavljajo posamezne akcije) le polja neposredno ob cilju. Podobno imajo tudi le ta polja pravilno obrnjene vektorje.}
\label{rstdp_3x3}
\end{figure}

\newpage
\section{TD učenje in model akter-kritik}
\label{sec:td_learning}
Časovno razlikovalno učenje (angl. \textit{Temporal Difference Learning}, \textit{TD}) je metoda spodbujevanega učenja, ki posodablja oceno vrednosti stanj ali parov stanje–akcija sproti, med neposredno interakcijo z okoljem. Ključna prednost TD-učenja je njegova sposobnost učenja iz oddaljenih nagrad, saj omogoča postopno razširjanje informacije o nagradi nazaj skozi zaporedje predhodnih stanj. Namesto čakanja na končni izid epizode (sekvence prehajanja stanj do prihoda v končno stanje) TD-učenje primerja trenutno oceno vrednosti s t. i. TD-napako $\delta_t$, ki temelji na naslednjem stanju in morebitni takojšnji nagradi. Na ta način se tudi dejanja, ki sama po sebi ne prinesejo takojšnje nagrade, a vodijo do kasnejšega uspeha, sčasoma ustrezno ovrednotijo.
\\
\\
\noindent Osnovna posodobitvena enačba za vrednostno funkcijo stanja:
\[
V(s_t) \leftarrow V(s_t) + \alpha\,\delta_t,
\]

\noindent kjer je \(\alpha\) hitrost učenja, TD-napaka \(\delta_t\) pa je definirana kot

\[
\delta_t = r_{t+1} + \gamma V(s_{t+1}) - V(s_t).
\]
\noindent V izrazu je \(r_{t+1}\) nagrada ob prehodu iz stanja \(s_t\) v stanje \(s_{t+1}\), faktor \(\gamma \in [0,1]\) pa določa relativno težo prihodnjih nagrad. TD-napaka predstavlja razliko med izboljšano napovedjo vrednosti in prejšnjo oceno.
\\
\\
\noindent V tem delu bomo implementirali algoritem TD(0), pri katerem se vrednosti posodabljajo izključno na podlagi trenutne nagrade in ocene vrednosti naslednjega stanja. Izbiro akcije v posameznem stanju bomo glede na nagrado posodabljali po enakem R-STDP mehanizmu kot doslej, pri čemer bomo izbire akcij nagrajevali s pomočjo sledi upravičenosti. Tako bo pravzaprav možno tudi, da nagradimo sinapse, ki so predstavljale izbire akcij v preteklih stanjih, kar je ideja splošnejše oblike TD-učenja -- metode TD($\lambda$). S tem, da stanja definiramo z 200 ms stimulacijo vhodnega nevrona, pa bomo z ustrezno izbrano konstanto $\tau_c$, ki določa odtekanje sledi upravičenosti, lahko dosegli minimalen vpliv sinaps, ki ne predstavljajo izbire akcije v trenutnem stanju, in se tako približali TD(0). To nam bo močno olajšalo spremljanje učenja in iskanje ustreznih hiperparametrov, kar je sicer pri impulznih nevronskih mrežah lahko zahtevno.

\subsection{Model akter-kritik}
Za TD učenje poleg že razvitega R-STDP modula za učenje pravilne izbire akcije potrebujemo še modul, ki računa TD-napako $\delta_t$. Takemu modelu pravimo akter-kritik (angl. \textit{actor-critic}). V tej nalogi skušamo doseči čim bolj biološko verjetne implementacije, zato želimo, da impulzna nevronska mreža sama računa TD-napako. Pri tem se bomo zgledovali po \href{https://en.wikipedia.org/wiki/Basal_ganglia}{bazalnih ganglijih}, prikazanih na sliki \ref{basal_ganglia}, skupini nevronov v človeških možganih, ki med drugim realizira obliko TD-učenja. Za poenostavitev in abstrakcijo kompleksnega sistema in mehanizmov, prisotnih v pravih bazalnih ganglijih, se bomo zgledovali po \cite{actorCritic}. Cilj sistema, ki ga bomo razvili v nadaljevanju, bo na nalogi z mrežo doseči propagacijo zakasnjene nagrade v stanja, ki postopno vodijo do cilja, in s tem učenje optimalne politike za navigacijo do nagrade.
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=0.6\textwidth]{figures/basal_ganglia}
\end{center}
\caption{Diagram skupin nevronov bazalnih ganglijev.}
\label{basal_ganglia}
\end{figure}
\newpage
Kot omenjeno, je model akter-kritik, kot ga predstavlja \cite{actorCritic}, poenostavitev in abstrakcija resničnih mehanizmov v možganih, ki to omogočajo. V bazalnih ganglijih, kot so prikazani na sliki \ref{basal_ganglia}, sodeluje več skupin nevronov in povezav, ki so v tem modelu logično združeni v \textit{korteks}, ki predstavlja vhodne nevrone, \textit{motorične nevrone}, ki predstavljajo izhodne nevrone in možne akcije, ter skupine nevronov kritika: \textit{striatum}, \textit{ventralni pallidum} in dopaminergične nevrone. \textit{Substantia nigra} in \textit{talamus} pravih bazalnih ganglijev sta funkcionalno združena v dopaminergične nevrone, ki projicirajo dopamin do povezav med vhodom in striatumom ter vhodom in izhodnimi motoričnimi nevroni. 
Tako v bazalnih ganglijih kot v modelu akter-kritik, kot ga predstavlja Wiebke P, et al., razlikujemo dve glavni poti: \textit{direktno} in \textit{indirektno} pot, ki vodita iz nevronov striatuma do dopaminergičnih nevronov. Direktna pot je zakasnjena inhibitorna pot, ki poteka neposredno iz striatuma do dopaminergičnih nevronov,
indirektna pot pa je inhibitorna do nevronov ventralnega palliduma, posebne skupine nevronov, ki inhibira aktivnost dopaminergičnih nevronov. Ventralni pallidum se nahaja ventralno od \textit{globusa pallidusa}, prikazanega na klasičnem diagramu bazalnih ganglijev, in je povezan s pričakovanjem nagrade in odločanjem, zato Wiebke P, et al. za skupino nevronov na indirektni poti verjetno izbere to poimenovanje. Opisan akter-kritik model je prikazan na sliki \ref{actor-critic}.
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=1.0\textwidth]{figures/actor-critic}
\end{center}
\caption{Model akter-kritik, kot ga predlaga \cite{actorCritic}.}
\label{actor-critic}
\end{figure}

\noindent Ob prisotnosti osnovne, stalno prisotne neničelne frekvence nevronov ventralnega palliduma, ki jo povzročimo z injiciranim Poissonovim šumom, bo aktivnost indirektne poti imela na dopaminergične nevrone vzbujajoč učinek. Aktivnost nevronov ventralnega palliduma namreč prek inhibitorne povezave znižuje aktivnost dopaminergičnih nevronov. Če prek inhibitorne povezave iz striatuma zmanjšamo aktivnost nevronov ventralnega palliduma, pa bo aktivnost dopaminergičnih nevronov zato narasla. Indirektna in direktna povezava na dopaminergične nevrone delujeta konkurenčno. Sinapse na indirektni poti imajo minimalen zamik zaradi prenašanja signala. Indirektna pot tako zvišano aktivnost striatuma v trenutnem stanju z minimalnim zamikom preslika v povišano aktivnost dopaminergičnih nevronov. Hkrati v času nahajanja v trenutnem stanju direktna povezava inhibira dopaminergične nevrone sorazmerno z aktivnostjo striatuma, vendar glede na tisto, kakršna je bila v prejšnjem stanju, saj je direktna pot zakasnjena. Indirektna in direktna povezava skupaj računata TD-napako $\delta_t$, ki v trenutnem stanju glede na izračunan približek vrednosti trenutnega stanja okrepi sinapse prejšnjega stanja, odgovorne za izbiro akcije, ki nas je pripeljala v trenutno stanje. Ta mehanizem je prikazan na sliki \ref{actor_critic_learning}. 

Povprečna teža sinaps med vhodnim nevronom $i$ in striatumom predstavlja pričakovano nagrado in približek vrednosti stanja $i$. Ob prehodu iz stanja z visoko povprečno utežjo sinaps do striatuma v stanje z nizko bo prevladala direktna povezava in bodo dopaminergični nevroni inhibirani, in obratno. Če se premaknemo v stanje s približno enako povprečno utežjo povezave do striatuma, se bosta direktna in indirektna povezava izničili, dopaminergični nevroni pa se bodo prožili s frekvenco, ki jo določa zunanji Poissonov šum. 

\subsubsection{Implementacija}
Skupino vhodnih nevronov oziroma korteks bo predstavljalo $N_{\text{in}}$ nevronov, ki bodo povezani z $N_a$ motoričnimi oziroma izhodnimi nevroni. V akterju bomo zaradi razlogov, navedenih v poglavju \ref{sec:rstdp}, uporabljali model nevrona z eksponentnim jedrom, v kritiku pa biološko bolj realistične nevrone z alfa jedrom. Ker bodo vhodni nevroni akterju in kritiku skupni, bomo med vhodnimi in izhodnimi nevroni dodali dodatni nivo $N_{\text{in}}$ vhodnih motoričnih nevronov, ki so z vhodnimi nevroni prek statičnih povezav z utežmi $w_{\text{in}\to\text{in, motor}}$ povezani po režimu \textit{one-to-one}, torej en vhodni nevron z enim nevronom vmesnega nivoja. Vmesni nivo nam bo omogočil prilagajanje frekvence in šuma, potrebnega za R-STDP učenje v akterju, ločeno od kritika, kar nam bo olajšalo iskanje ustreznih hiperparametrov. Vmesni nevroni so po režimu \textit{all-to-all} (vsak vhodni nevron je povezan z vsakim izhodnim) povezani z izhodnimi nevroni prek zakasnjenih sinaps R-STDP z normalno porazdeljenimi utežmi. Prav tako so vhodni nevroni po režimu \textit{all-to-all} povezani z $N_{\text{kritik}}$ nevroni striatuma prek zakasnjenih sinaps R-STDP z normalno porazdeljenimi utežmi. Striatum je prek statičnih inhibitornih povezav z utežmi $w_{\text{str}\to\text{vp}}$ povezan z $N_{\text{kritik}}$ nevroni ventralnega palliduma in prek statičnih inhibitornih povezav z utežmi $w_{\text{str}\to\text{dopa}}$ ter zakasnitvijo $d_{\text{dir}}$ z $N_{\text{dopa}}$ dopaminergičnimi nevroni. 
Ventralni pallidum je z dopaminergičnimi nevroni prav tako povezan prek statičnih inhibitornih povezav, ki pa niso zakasnjene in imajo uteži $w_{\text{vp}\to\text{dopa}}$. Vse povezave kritika so povezane po režimu \textit{all-to-all}. V nevrone vmesnega nivoja med vhodnimi in izhodnimi nevroni injiciramo Poissonov šum s povprečno hitrostjo $\lambda_{\text{in, motor}}$, v izhodne nevrone pa Poissonov šum s hitrostjo $\lambda_{\text{out, motor}}$. Poissonov šum prav tako injiciramo v nevrone ventralnega palliduma s povprečno hitrostjo $\lambda_{\text{vp}}$ in v dopaminergične nevrone s povprečno hitrostjo $\lambda_{\text{dopa}}$. 
Generatorji Poissonovega šuma so s posameznimi skupinami nevronov povezani prek statičnih povezav z utežmi $w_{\text{ext, in}}$, $w_{\text{ext, out}}$, $w_{\text{ext, vp}}$ in $w_{\text{ext, dopa}}$. Hiperparametri sinaps R-STDP in modelov nevronov so skupaj z ostalimi hiperparametri navedeni v poglavju \ref{sec:izbira_parametrov}. Opisani model je prikazan na sliki \ref{actor_critic_diagram}.
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=1.0\textwidth]{figures/dipl_TD_model}
\end{center}
\caption{Prikaz implementiranega sistema akter-kritik, kjer okolje v iteraciji $i$ sistemu signalizira stanje $s_i$ s stimulacijo ustreznega vhodnega nevrona. Akter izbere akcijo $a_i$, okolje pa ob prehodu v novo stanje stimulira dopaminergične nevrone s tokom $I_{R, i-1}$, ki predstavlja potencialno nagrado.}
\label{actor_critic_diagram}
\end{figure}

Za razliko od izvornega sistema (\cite{actorCritic}), bomo za model sinapse uporabili našo zakasnjeno sinapso R-STDP, kot smo jo razvili v poglavjih \ref{sec:synapse_model} in \ref{sec:rstdp}, ki poleg presinaptičnih impulzov upošteva tudi postsinaptične po pravilu STDP. Tako bomo lahko kot akter uporabili sistem R-STDP, ki smo ga razvili v poglavju \ref{sec:rstdp}. Naša implementacija se razlikuje tudi v načinu izbire akcije, saj za izbrano akcijo izberemo tisto, katere pripadajoči izhodni nevron ima najvišje število impulzov v trenutnem stanju. V izvornem sistemu je akcija izbrana glede na prvi impulz pripadajočega izhodnega nevrona, ki se sproži kot rezultat stimulacije v trenutnem stanju. Pri tem načinu moramo po prvem impulzu v čim krajšem časovnem intervalu inhibirati vse ostale izhodne nevrone. Tako bomo bolj neposredno okrepili povezave, odgovorne za izbrano aktivnost, saj smo z inhibicijo dosegli ničelne sledi upravičenosti sinaps do ostalih izhodnih nevronov, ker se ti ne bodo prožili. V tem primeru tudi ne bo potrebe po tekmovanju sinaps, vendar moramo zato preveriti impulze izhodnih nevronov v vsakem koraku simulatorja. V primeru simulatorja NEST je to vsakih 0,1 ms, kar je problematično, saj simulator teče v C++ zaledju, ki ga zapustimo takoj, ko prekinemo simulacijo. Tako je bistvena razlika med tem, ali 100-krat poženemo ukaz \texttt{nest.Simulate(0.1)} ali enkrat \texttt{nest.Simulate(10)}. Naš sistem bo zaradi hitrosti simulacije po številu nevronov manjši od izvornega sistema.
\subsection{Izbira parametrov}
\label{sec:izbira_parametrov}
Parametri so bili izbrani eksperimentalno, tokrat brez oziranja na biološko točnost. Pri spreminjanju velikosti posameznih skupin nevronov moramo pri izbiri parametrov paziti, da ohranjamo osnovno frekvenco dopaminergičnih nevronov ter ravnovesje med inhibicijo in vzbujanjem zaradi direktne in indirektne povezave. Zmanjšanje frekvence, ki jo izvaja plast nevronov med vhodnimi in izhodnimi nevroni, mora biti dovolj veliko, da šum pri osnovnih utežeh sinaps med srednjo plastjo in izhodnimi nevroni omogoči učenje, kot je razloženo v poglavju \ref{sec:rstdp}.
V tabelah \ref{tab:params1}-\ref{tab:params6} so navedene konstante in parametri implementiranega modela. Parametri, ki niso prikazani v tabelah, imajo privzete vrednosti simulatorja NEST.

\begin{table}[htbp]
    \label{tab:params1}
\begin{tabular}{lll}
\textbf{Simbol} & \textbf{Pomen} & \textbf{Vrednost} \\ \hline
% ------- Simulation constants -------
%TODO: number of neurons
POLL\_TIME & Čas simulacije na iteracijo & 200 \\ \hline
\(f(s_{\text{in}, i})\) & frekvenca stimulacije vhodnega nevrona $i$ & $100$ Hz \\ \hline
%\(n_{\text{kritik}}\) & Število nevronov v skupinah nevronov kritika & 8 \\ \hline
\end{tabular}
\caption{Parametri simulacije}
\end{table}
\\
\begin{table}[htbp]
    \label{tab:params2}
\begin{tabular}{lll}
\textbf{Simbol} & \textbf{Pomen} & \textbf{Vrednost} \\ \hline
\thead[l]{\bfseries Parametri skupin \\
\bfseries nevronov kritika} \\
tip & Tip modela nevrona & \textit{iaf\_psc\_alpha} \\ \hline
\(C_{m,\text{in}}\) & Membranska kapacitivnost & 250.0 pF \\ \hline
\(\tau_{m,\text{in}}\) & Časovna konstanta membrane & 10.0 ms \\ \hline
\(V_{\text{reset,in}}\) & Potencial ponastavitve & 0.0 mV \\ \hline
\(V_{\text{th,in}}\) & Prag proženja & 20.0 mV \\ \hline
\(t_{\text{ref,in}}\) & Refraktorna doba & 0.5 ms \\ \hline
\(\tau_{\text{syn,ex,in}}=\tau_{\text{syn,in,in}}\) & \makecell[l]{Vzbujajoča in inhibitorna \\ sinaptična konstanta} & 2 ms \\ \hline
\(\tau_{\text{-,a}}\) & Negativna STDP konstanta & 20.0 ms \\ \hline
\(V_{m,\text{in}}\) & Začetni membranski potencial & 0.0 mV \\ \hline
\(E_{L,\text{in}}\) & Mirovalni potencial & 0.0 mV \\ \hline
\\
\thead[l]{\bfseries Parametri motoričnih \\
\bfseries nevronov} \\
tip & Tip modela nevrona & \textit{iaf\_psc\_exp} \\ \hline
\(C_{m,a}\) & \makecell[l]{Membranska kapacitivnost \\ motornih nevronov} & 250.0 pF \\ \hline
\(\tau_{m,a}\) & Časovna konstanta membrane & 10.0 ms \\ \hline
\(V_{\text{reset,a}}\) & Potencial ponastavitve & 0.0 mV \\ \hline
\(V_{\text{th,a}}\) & Prag proženja & 20.0 mV \\ \hline
\(t_{\text{ref,a}}\) & Refraktorna doba & 0.1 ms \\ \hline
\(\tau_{\text{syn,ex,a}}=\tau_{\text{syn,in,a}}\) & \makecell[l]{Vzbujajoča in inhibitorna \\ sinaptična konstanta} & 2 ms \\ \hline
\(\tau_{\text{-,a}}\) & Negativna STDP konstanta & 20.0 ms \\ \hline
\(V_{m,a}\) & Začetni membranski potencial & 0.0 mV \\ \hline
\(E_{L,a}\) & Mirovalni potencial & 0.0 mV \\ \hline
\end{tabular}
\caption{Parametri nevronov}
\end{table}

\\
\begin{table}[htbp]
    \label{tab:params3}
\begin{tabular}{lll}
\textbf{Simbol} & \textbf{Pomen} & \textbf{Vrednost} \\ \hline
\\
\thead[l]{\bfseries Parametri sinaps med \\
\bfseries vhodnimi in \\
\bfseries vhodnimi motoričnimi \\
\bfseries nevroni} \\ \hline
tip & Tip sinapse & \makecell[l]{Privzeta \\ konstantna \\ sinapsa NEST} \\ \hline
\(w_{\text{in}\to\text{in, motor}}\) & \makecell[l]{Uteži sinaps med \\ vhodnimi in vhodnimi \\ motoričnimi nevroni} & 120 \\ \hline
\\
\thead[l]{\bfseries Parametri sinaps med \\
\bfseries vhodnimi in \\
\bfseries izhodnimi motoričnimi \\
\bfseries nevroni} \\ \hline
% ------- STDP parameters (delayed_synapse) -------
tip & Tip sinapse & \makecell[l]{Zakasnjena \\ sinapsa R-STDP} \\ \hline
\(\tau_{c}\) & Odtekanje sledi upravičenosti & 5 ms \\ \hline
\(\tau_{c\text{, delay}}\) & Zakasnitev sledi $c$ & 200 ms \\ \hline
\(\tau_{n}\) & Odtekanje dopaminske sledi & 10 ms \\ \hline
\(\tau_{+}\) & Pozitivna STDP konstanta & 20 ms \\ \hline
\(b\) & \makecell[l]{Bazalna dopaminska \\ koncentracija} & 0.1  \\ \hline
\(A_{+}\) & Pozitivni STDP multiplikator & 1.5 \\ \hline
\(A_{-}\) & Negativni STDP multiplikator & 1.0 \\ \hline
\(W_{\min,a}\) & Minimalna utež & 500 \\ \hline
\(W_{\max,a}\) & Maksimalna utež & 4000 \\ \hline
\(w_{\text{in, motor}\to a}\) & \makecell[l]{Začetne uteži sinaps med \\ vhodnimi in izhodnimi \\ motoričnimi nevroni} & \(\mathcal{N}(1300, 1)\) \\ \hline
\end{tabular}
\caption{Parametri sinaps med vhodnimi in motoričnimi nevroni}
\end{table}
\\
\begin{table}[htbp]
    \label{tab:params4}
\begin{tabular}{lll}
\textbf{Simbol} & \textbf{Pomen} & \textbf{Vrednost} \\ \hline
\thead[l]{\bfseries Parametri sinaps med \\
\bfseries vhodnimi nevroni \\
\bfseries in striatumom} \\ \hline
% ------- STDP parameters (delayed_synapse) -------
tip & Tip sinapse & \makecell[l]{Zakasnjena \\ sinapsa R-STDP} \\ \hline
\(\tau_{c}\) & Odtekanje sledi upravičenosti & 5 ms \\ \hline
\(\tau_{c\text{, delay}}\) & Zakasnitev sledi $c$ & 200 ms \\ \hline
\(\tau_{n}\) & Odtekanje dopaminske sledi & 10 ms \\ \hline
\(\tau_{+}\) & Pozitivna STDP konstanta & 20 ms \\ \hline
\(b\) & \makecell[l]{Bazalna dopaminska \\ koncentracija} & 0.1  \\ \hline
\(A_{+}\) & Pozitivni STDP multiplikator & 1.5 \\ \hline
\(A_{-}\) & Negativni STDP multiplikator & 1.0 \\ \hline
\(W_{\min,str}\) & Minimalna utež & 150 \\ \hline
\(W_{\max,str}\) & Maksimalna utež & 1000 \\ \hline
\(w_{\text{in}\to \text{str}}\) & \makecell[l]{Začetne uteži sinaps med \\ vhodnimi in striatum nevroni} & \(\mathcal{N}(150, 8)\) \\ \hline

% ------- Synaptic weights (key model pathways) -------
\end{tabular}
\caption{Parametri sinaps med vhodom in striatumom}
\end{table}
\begin{table}[htbp]
    \label{tab:params5}
\begin{tabular}{lll}
\textbf{Simbol} & \textbf{Pomen} & \textbf{Vrednost} \\ \hline
tip & Tip sinapse & \makecell[l]{Privzeta konstantna \\ sinapsa NEST} \\ \hline
\(w_{\text{str}\to\text{vp}}\) & \makecell[l]{Uteži sinps med striatumom in \\ ventral pallidumom} & -50 \\ \hline
\(w_{\text{str}\to\text{dopa}}\) & \makecell[l]{Uteži sinps med striatumom in \\ dopaminergičnimi nevroni} & -55 \\ \hline
\(w_{\text{vp}\to\text{dopa}}\) & \makecell[l]{Uteži sinps med \\ ventral pallidumom in \\ dopaminergičnimi nevroni} & -65 \\ \hline
\(d_{\text{dir}}\) & \makecell[l]{Zakasnitev sinaps \\ direktne povezave} & 200 ms \\ \hline
\end{tabular}
\caption{Parametri sinaps kritika}
\end{table}
\begin{table}[htbp]
    \label{tab:params6}
\begin{tabular}{lll}
\textbf{Simbol} & \textbf{Pomen} & \textbf{Vrednost} \\ \hline
% ------- External noise and background -------
\(\lambda_{\text{vp}}\) & \makecell[l]{Povprečna hitrost šumnih impulzov \\ nevronov ventral palliduma} & 5200 \\ \hline
\(\lambda_{\text{dopa}}\) & \makecell[l]{Povprečna hitrost šumnih impulzov \\ dopaminergičnih nevronov} & 4000 \\ \hline
\(\lambda_{\text{in, motor}}\) & \makecell[l]{Povprečna hitrost šumnih impulzov \\ vhodnih motoričnih nevronov} & $100$ Hz \\ \hline
\(\lambda_{\text{out, motor}}\) & \makecell[l]{Povprečna hitrost šumnih impulzov \\ izhodnih motoričnih nevronov} & $100$ Hz \\ \hline
\(w_{\text{ext, in}}\) & \makecell[l]{Uteži statičnih povezav \\ med generatorjem \\ Poissonovega šuma in \\ vhodnimi motoričnimi nevroni} & 50 \\ \hline
\(w_{\text{ext, out}}\) & \makecell[l]{Uteži statičnih povezav \\ med generatorjem \\ Poissonovega šuma in \\ izhodnimi motoričnimi nevroni} & 50 \\ \hline
\(w_{\text{ext, vp}}\) & \makecell[l]{Uteži statičnih povezav \\ med generatorjem \\ Poissonovega šuma in \\ nevroni ventralnega palliduma} & 50 \\ \hline
\(w_{\text{ext, dopa}}\) & \makecell[l]{Uteži statičnih povezav \\ med generatorjem \\ Poissonovega šuma in \\ dopaminergičnimi nevroni} & 50 \\ \hline
\end{tabular}
\caption{Parametri generatorjev šuma}
\end{table}
\\
\subsection{Učenje}
Posamezno stanje $i$ bo za akterja predstavljala 200 ms stimulacija vhodnega nevrona $i$, enako kot v poglavju \ref{sec:rstdp}. 
\iffalse
Pri prehodu med stanji se ob dovolj visoki koncentraciji dopamina lahko okrepijo tudi sinapse do izhodnih nevronov, ki so povezane z vhodnimi nevroni prejšnjega stanja, saj lahko \textit{eligibility} sledi ostanejo večje od 0 tudi prek več prehodov stanj. To v osnovi ni napačno in je posledica uporabe RSTDP sinapse, vendar bomo za učinkovitejše učenje med prehodi stanj prekinili stimulacijo 50 ms pred stimulacijo novega stanja, saj so za našo nalogo stanja med seboj neodvisna. Za določeno stanje ni pomembno, v katerem stanju smo bili prej. 
\fi
Mehanizme, opisane v prejšnjem poglavju, najprej preverimo na sekvenci prehodov med dvema stanjema 0 in 1. Začeli bomo v stanju 0, se premaknili v stanje 1, kjer agent prejme nagrado, nato pa se vrnemo nazaj v stanje 0, kjer ostanemo. Aktivnosti posameznih skupin nevronov in posodabljanje uteži sinaps akterja in kritika so prikazane na sliki \ref{actor_critic_learning}. Ob prehodu v nagrajeno stanje 1 vidimo, da se uteži povezav med vhodnim nevronom, ki predstavljaja stanje 0 in striatumom okrepijo. To predstavlja zvišanje pričakovane nagrade v stanju 0, nagrada, ki smo jo dovedli ob prehodu v nagrajeno stanje 1, pa ne pomeni, da tudi v stanju 1 pričakujemo visoko nagrado. Pričakovana nagrada se namreč zviša ob prehodu v stanje z višjo vrednostjo ali ob zunanji nagradi in je odvisna od akcije, ki jo izvedemo. Ob prehodu iz stanja 1 nazaj v stanje 0 prehajamo iz stanja z osnovnimo pričakovano nagrado v stanje 0, ki ima tokrat okrepljene uteži do striatuma in povišano pričakovano nagrado. To predstavlja prehod v stanje z višjo vrednostjo, posledica tega pa je, v primerjavi z osnovno frekvenco dopaminergičnih nevronov, zvišana dopaminergična aktivnost. Ta povzroči sorazmerno povišanje uteži sinaps do striatuma v stanju 1. V nadaljevanju ostajamo v stanju 0, kjer se ob prehodu iz stanja 0 v stanje 0 vrnemo k osnovni dopaminergični frekvenci, saj se vpliv direktne in indirektne povezave izničita.

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=1.0\textwidth]{actorcritic/F2 - StateTransitionTD}
\end{center}
\caption{Prikaz posodabljanja sinaptičnih uteži ob prehajanju med dvemi stanji. Na grafu povprečne aktivnosti nevronov striatuma spremljamo vrednost trenutnega stanja oziroma pričakovano nagrado. Zvišana aktivnost striatuma brez zamika predstavlja znižano aktivnost ventralnega palliduma. Vsota aktivnosti ventralnega palliduma in aktivnosti striatuma zamaknjene za čas enega stanja (200 ms) v preteklost pa predstavlja aktivnost dopaminergičnih nevronov in dovedeno nagrado v tem stanju. Ker uporabljamo zakasnjeno sinapso R-STDP, bodo ob zvišani dopaminergični aktivnosti okrepljene povezave iz vhodnih nevronov prejšnjega stanja. V intervalu med 200 ms in 400 ms, kjer dovedemo zunanjo nagrado ob prihodu v stanje 1, se bodo okrepile povezave iz vhodnega nevrona 0 do izhodnih nevronov in striatuma. Povezave iz posameznega vhodnega nevrona do striatuma se okrepijo enakomerno, medtem ko pri krepljenju povezav do izhodnih nevronov poteka tekmovanje med povezavami in učenje politike prek R-STDP učenja, kot opisano v poglavju \ref{sec:rstdp}.}
\label{actor_critic_learning}
\end{figure}

\newpage
\subsection{Rezultati}
Naučeno politiko bomo prikazali podobno kot v poglavju \ref{sec:rstdp}, vendar bomo namesto maksimalne razlike med utežmi povezav do različnih akcij uporabili kar povprečno utež povezave od vhodnega nevrona stanja do striatuma. Stanja z največjimi utežmi do striatuma oziroma stanja z najvišjo pričakovano nagrado, so bila tekom učenja deležna največ nagrajevanja, zato pričakujemo, da so v teh stanjih akcije najbolj diferecirane. 

Rezultat učenja na 4x4 mreži po 3000 iteracijah je prikazan na sliki \ref{results_4x4}. Izbira akcije v posameznem polju je razvidna iz smeri puščice, kjer vidimo, da se je agent naučil skoraj optimalne navigacije do cilja iz poljubnega stanja. Pričakovano imajo stanja neposredno ob nagrajenem stanju najvišjo pričakovano vrednost, dlje kot se oddaljimo od tega stanja, nižja je pričakovana nagrada. Stanja, ki so najbolj oddaljena od cilja, ima pri trenutni izbiri parametrov minimalno pričakovano nagrado. Propagiranje pričakovane nagrade od končnega stanja lahko pospešimo tako, da povišamo amplitudo posodobitve povezav do striatuma, vendar bomo s tem zvišali tudi rast motoričnih sinaps. Te bodo rasle prehitro, zato tekmovanje sinaps, kot je opisano v poglavju \ref{sec:rstdp}, ne bo tako učinkovito. Z drugimi besedami, stanja bomo nagrajevali prehitro.
\clearpage

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=0.6\textwidth]{actorcritic/best}
\end{center}
\caption{Prikaz politike modela na 4x4 mreži po 3000 iteracijah po 200 ms. Končno stanje je obarvano z zeleno. Barva ostalih stanj prikazuje povprečno utež povezav med vhodnim nevronom, ki predstavlja to stanje in striatumom, oziroma pričakovano nagrado. V vsakem polju je s pučico prikazan vektor preferirane akcije (smeri) glede na uteži med vhodnim nevronom in posameznimi izhodnimi nevroni. Vidimo, da pričakovana nagrada od končnega stanja, propagira v bolj oddaljena stanja. Smeri vektorjev stanj kažejo v smeri najboljše akcije, kar kaže na uspešno učenje, z izjemo najbolj oddaljenih stanj, kjer se sorazmerno s pričakovano nagrado agent ni naučili optimalne politike.}
\label{results_4x4}
\end{figure}

Podobno kot pri R-STDP bomo učenje spremljali s povprečno nagrado, vendar tokrat k nagradi ne bo prispevala le zunanja nagrada, temveč tudi pričakovane nagrade. Pričakujemo, da bodo pričakovane nagrade med učenjem v vseh stanjih naraščale. To preverimo z neposrednejšo evalvacijo učenja na mreži, kjer po vsaki ponastavitvi stanja (ko dosežemo cilj) štejemo korake, dokler ponovno ne pridemo v ciljno stanje. Ker so različna stanja, v katera naključno postavimo agenta, različno oddaljena od cilja, bomo število korakov delili z manhattansko razdaljo do cilja. Tako dobimo ``relativne korake''. Povprečno število relativnih korakov glede na povprečno utež vseh povezav med vhodnimi nevroni in nevroni striatuma tekom 3000 iteracij je prikazano na grafu \ref{progress}, kjer vidimo, da povprečno število korakov, ki jih agent potrebuje, da pride do cilja, pada sorazmerno z rastjo povprečne pričakovane nagrade.

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=1.0\textwidth]{actorcritic/progress}
\end{center}
\caption{Povprečna utež sinaps med vsemi vhodnimi nevroni in striatumom tekom 3000 iteracij po 200 ms ter povprečno relativno število korakov. Na sliki vidimo, da medtem ko povprečna utež sinaps med vsemi vhodnimi nevroni in striatumom, ki predstavlja globalno pričakovano nagrado glede na naučeno politiko, raste, povprečno relativno število korakov pada, kar je odraz uspešnega učenja politike, ki se približuje optimalni.}
\label{progress}
\end{figure}

Sistem, kot smo ga implementirali izvaja t. i. ``on-policy'' TD-učenje, saj vrednost posameznih stanj posodablja glede na vrednost naslednjega stanja, kjer naslednjo akcijo izberemo glede na naučeno politiko, za razliko od t. i. ``off-policy'' algoritmov, kjer vrednosti posodabljamo ob predpostavki, da bomo v naslednjem stanju vedno izbrali najboljšo akcijo.

\chapter{Zaključek}
Namen te diplomske naloge je bil predstaviti in rešiti nekatere izzive pri učenju impulznih nevronskih mrež. V nalogi razvijemo inovativne rešitve, ki upoštevajo tako zahtevnost simulacije, kot tudi smiselnost z vidika nevrologije in resničnih mehanizmov v možganih. Dodatno smo se izognili vpeljavi negativne nagrade oziroma negativne koncentracije dopamina, saj to v resničnih možganih ni mogoče. Tako smo razvili R-STDP sistem, ki temelji zgolj na tekmovanju med sinapsami, za probleme, kjer se želimo določenim stanjem izogibati, pa predlagamo razširitev, ki ne uporablja negativne koncentracije dopamina. Od biološko realističnega sistema se najbolj oddaljimo pri iskanju parametrov sistema. Parametre smo iskali le z vidika doseganja želenih mehanizmov, ne pa tudi skladnosti z vrednostmi, izmerjenimi v resničnih možganih. To si dopuščamo tudi zato, ker se način proženja nevronov, prenos signalov po sinapsah ter konfiguracije nevronov, kot so v bazalnih ganglijih zdijo, bolj skupni različnim živalskim vrstam kot parametri nevronov in sinaps. Center, odgovoren za sluh in modulacijo glasilk, je na primer po strukturi pri človeku in netopirju podoben, vendar pri netopirju ti centri očitno delujejo na precej višji frekvenci. %TODO: preveri 
Nadaljnje iskanje hiperparametrov bi najverjetneje lahko privedlo do boljših rezultatov, kot so bili doseženi v tej diplomski nalogi. Pri sistemih brez rekurenčnih povezav in brez dodatnih popolno povezanih plasti nevronov, kakršni so sistemi implementirani v tej nalogi, višanje števila nevronov v posamezni skupini ne bi nujno privedlo do boljših rezultatov, vendar to ni bilo preverjeno zaradi računske zahtevnosti.

\section{Ideje za nadaljnje delo}
\subsection{Izognitveno obnašanje (angl. \textit{aversive behaviour})}
\label{aversive}
V večini del, ki se ukvarjajo s spodbujevanim učenjem, se izogibanje stanjem, za katere želimo, da se jih agent izogiba, doseže s pomočjo negativne nagrade. Negativna nagrada v enačbi sinapse R-STDP obrne predznak posodobitve. Tako so sinapse, odgovorne za vstop v neželeno stanje, negativno posodobljene. V človeških možganih negativnega dopamina ni. Pojavi se ideja, da je izogibanje negativnim stanjem prav tako posledica učenja, kjer je nivo dopamina $n > 0$. Dopamin namreč predstavlja učenje, ne nujno nagrade. Negativno nagrado bi tako lahko predstavili s posebnim vhodom, ki predstavlja nek negativen stimulus ali ``neugodje'', ki ga tekom učenja želimo zmanjšati. Zopet lahko uporabimo načela R-STDP in TD učenja, kjer zmanjšanje nivoja neugodja predstavlja nagrado. Trenutnemu akter-kritik sistemu bi dodali še eno instanco kritika, ki računa časovno razliko nivoja neugodja in deluje na dopaminergične nevrone, ki so skupni obema kritikoma. Oba kritika tako delujeta konkurenčno. Ob prehodu iz stanja z visokim nivojem negativnega stimulusa v stanje z nizkim, dopaminergične nevrone vzbudimo, v obratnem primeru pa inhibiramo. V primeru enakega nivoja dovedenega negativnega stimulusa kritik negativne nagrade ne vpliva na dopaminergične nevrone. 

Sistem se do negativnega stimulusa v tem primeru, kljub uporabi časovne razlike, obnaša kratkovidno. Nagrajene bodo samo povezave, ki so nas vodile stran od neugodja, ker pa je negativen stimulus vedno doveden samo iz zunanjosti sistema, bodo nagrajene povezave le v stanja neposredno ob negativnem stanju. Striatum za razliko od tega nivo dovedene nagrade napoveduje sam. Če želimo v stanjih blizu neugodnega stanja neugodje tudi predvidevati, bi morali v sistem dodati še skupino nevronov, ki stanja asociirajo z negativnim stimulusom in ga tako napovedujejo.

Pričakujemo, da bi oba kritika med seboj tekmovala za nagrajevanje tako akcij, ki vodijo bližje nagradi, kot tudi tistih, ki vodijo stran od negativnega stanja.
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=0.4\textwidth]{figures/aversive}
\end{center}
\caption{Pričakovana politika ob kritiku negativnih stanj (brez kritika nagrajenih stanj).}
\label{pic8}
\end{figure}
\subsection{Rekurenčne povezave}
Velika predpostavka sistemov, razvitih v tej diplomski nalogi, je, da rekurenčnih povezav ni. Tako so stanja časovno med seboj skoraj popolnoma neodvisna. Če dodamo več vmesnih nivojev in rekurenčne povezave, bodo stanja med seboj postala časovno odvisna. Pravzaprav stanja ne moremo več definirati samo z aktivnostjo vhodnih nevronov, saj v vsakem trenutku stanje vsebuje tudi informacijo iz nevronov, ki so se prožili arbitrarno v preteklosti in nosijo informacijo o nekem prejšnjem stanju. V primeru našega akter-kritik sistema bi tako v vsakem trenutku $t$ kritik računal časovno razliko med dvema neskončno kratkima stanjema $s_t$ in $s_{t-d}$, kjer je $d$ zakasnitev direktne povezave. Kljub temu pričakujemo, da rezultat ne bi bil drugačen saj bi ob prisotnosti 200 ms stimulacije, ki je do zdaj predstavljala stanje, v tem intervalu vseeno prevladala nevronska aktivnost, ki je neposredna posledica stimulacije vhodnih nevronov.

V dosedanjih eksperimentih pravilna akcija določenega stanja ni bila odvisna od akcij, ki so nas pripeljale v to stanje, oziroma od zgodovine stanj. V primeru sprehajanja po mreži bomo končno stanje nagradili ne glede na to, iz katerega stanja vstopimo v nagrajeno stanje. Pričakujemo, da bi rekurenčne povezave predstavljale prednost pri nalogah, kjer je zgodovina stanj pomembna, oziroma kjer je nagrada stanja odvisna od prejšnjih stanj. Če bi v primeru sprehajanja po mreži premik v končno stanje iz stanja nad njim pripeljal do nagrade, prehod iz stanja levo pa ne, bi lahko tako končno stanje obravnavali kot dve različni stanji, glede na prehod. Sistem z rekurenčnimi povezavami bi kljub temu, da prejema le informacijo o konkretnem polju, interno predstavljal stanja odvisna tudi od prejšnjih premikov.

Rekurenčne povezave pa predstavljajo tudi dodaten izziv. V primeru našega akter-kritik sistema bi bila na primer potrebna redefinicija trenutnega načina izbire akcij, saj sta lahko dva izhodna nevrona povezana med seboj in se bosta vedno prožila skupaj. Rešitev bi lahko bila dopuščanje izbire več akcij hkrati, kjer takšno situacijo ``kaznujemo''.


\iffalse
\chapter{Ekstra}
\begin{figure}[H]
\begin{center}
\includegraphics[width=1.0\textwidth]{actorcritic/F3 - Learning}
\end{center}
\caption{Obnašanje sistema tekom učenja na 3x3 mreži. Polja so oštevilčena od leve proti desni od zgoraj navzdol. Cilj se nahaja na polju 8. Povezave vhoda do striatuma stanj 5 in 7 so pričakovano najvišje, sledi pa jim 4, ki neposredno vodi v 5 in 7}
\\
\colorbox{Apricot}{%
  \parbox{0.9\linewidth}{%
    \textbf{
      Iz zgornje kolekcije grafov izberi izseke, ki predstavljajo ključne situacije med učenjem opisane mehanizme ocenjevanja nagrade in učenja.
    }%
  }%
}

\label{pic8}
\end{figure}

%\cleardoublepage
%\addcontentsline{toc}{chapter}{Literatura}

%\printbibliography[heading=bibintoc,type=article,title={Članki v revijah}]
%https://www.overleaf.com/project/609ce2055f917cb2f776732e
%\printbibliography[heading=bibintoc,type=inproceedings,title={Članki v zbornikih}]
%\printbibliography[heading=bibintoc,type=incollection,title={Poglavja v knjigah}]
\fi
\printbibliography[heading=bibintoc,title={Viri}]


\end{document}

